{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the BQ SQL Performance Challenge!\n",
        "> Authors:\n",
        "> - Ashish Modi (asmodi@google.com)\n",
        "> - Eri Santos (erisantos@google.com)\n",
        "> - Ruben Fernandez (fernandezruben@google.com)\n",
        "\n",
        "> Chopping board:\n",
        "> - Vadim Zaripov (vadz@google.com)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TG8eEBaYupuU"
      },
      "id": "TG8eEBaYupuU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instructions\n",
        "**IMPORTANT:**\n",
        "\n",
        "* **This lab must be run in Argolis logged in as `admin@<yourldap>.altostrat.com`**\n",
        "* Before running this lab, please [**follow the click-to-deploy of this demo**](https://cloud-demo-hub.corp.google.com/demo/Argolis-Data-Share/618).\n",
        "  * Deploying it will add the user `admin@<yourldap>.altostrat.com` to a group that has access to the dataset we'll be using.\n",
        "* Run the cells under this **Setup** section to prepare the project for the lab work.\n",
        "* This lab will create BQ reservations and assignments. Remember to run the *Cleanup* cells after you're done to remove them.\n",
        "\n",
        "#### Why Argolis? - About the data\n",
        "The lab uses a dataset shared only to the `admin@<yourldap>.altostrat.com` user (via BQ Sharing) after deploying the [go/demos demo mentioned above]((https://cloud-demo-hub.corp.google.com/demo/Argolis-Data-Share/618)). You can find the [BigQuery Sharing listing here](https://console.cloud.google.com/bigquery/analytics-hub/exchanges/projects/720965328418/locations/us/dataExchanges/argolis_shared_data_1840b70ffcb/listings/bigquery_optimization_lab_199a6715d4a).\n",
        "\n",
        " If you want to use different user, you can first link the dataset as `admin@`, and then share accordingly. Outside of the BQ Sharing linking steps, the rest of the notebook can run as the user of your choice."
      ],
      "metadata": {
        "id": "T2CtMboqrCYc"
      },
      "id": "T2CtMboqrCYc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "AedrB1uIvLw7"
      },
      "id": "AedrB1uIvLw7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables for the lab\n",
        "\n",
        "YOUR_PROJECT_ID = \"YOUR_PROJECT_ID\" # @param {type:\"string\"} # << == INPUT your bq project name for this lab\n",
        "LINKED_DATASET_ID = \"linked_sme_da\"  # @param {type:\"string\"} # << == INPUT name you want for your linked dataset\n",
        "YOUR_WORKING_DATASET = \"sme_da_lab_working_dataset\" # @param {type:\"string\"} # << == INPUT name you want for your local working dataset"
      ],
      "metadata": {
        "id": "hfDxFqdN-PmP"
      },
      "id": "hfDxFqdN-PmP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "from google.api_core import exceptions\n",
        "import humanize\n",
        "import requests\n",
        "import json"
      ],
      "metadata": {
        "id": "PD4rZHXFvwR7"
      },
      "id": "PD4rZHXFvwR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not modify\n",
        "PROJECT_ID = YOUR_PROJECT_ID\n",
        "\n",
        "# Do not modify\n",
        "REGION = \"US\"\n",
        "\n",
        "RESERVATION_NAME = \"sme-academy-reservation\"\n",
        "reservation_url = f\"projects/{PROJECT_ID}/locations/{REGION}/reservations/{RESERVATION_NAME}\"\n",
        "\n",
        "LISTING_ENDPOINT = \"https://analyticshub.googleapis.com/v1/projects/720965328418/locations/us/dataExchanges/argolis_shared_data_1840b70ffcb/listings/bigquery_optimization_lab_199a6715d4a:subscribe\"\n"
      ],
      "metadata": {
        "id": "VEBc7Ez7mH0m"
      },
      "id": "VEBc7Ez7mH0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subscribing to the dataset via BigQuery Sharing"
      ],
      "metadata": {
        "id": "rq9tA4X3qZ0M"
      },
      "id": "rq9tA4X3qZ0M"
    },
    {
      "cell_type": "code",
      "source": [
        "def subscribe_to_listing(listing_endpoint, project_id, linked_dataset_id, location):\n",
        "    \"\"\"\n",
        "    Subscribes to a BigQuery Analytics Hub listing via the API.\n",
        "\n",
        "    Args:\n",
        "        listing_endpoint (str): The API endpoint for the listing subscription.\n",
        "        project_id (str): The ID of the project where the destination dataset will be created.\n",
        "        linked_dataset_id (str): The ID of the destination dataset.\n",
        "        location (str): The location of the destination dataset.\n",
        "\n",
        "    Returns:\n",
        "        requests.Response: The response object from the API call.\n",
        "    \"\"\"\n",
        "    credentials, project = google.auth.default()\n",
        "    authed_session = google.auth.transport.requests.AuthorizedSession(credentials)\n",
        "\n",
        "    payload = {\n",
        "        \"destinationDataset\": {\n",
        "            \"datasetReference\": {\n",
        "                \"projectId\": project_id,\n",
        "                \"datasetId\": linked_dataset_id\n",
        "            },\n",
        "            \"location\": location\n",
        "        }\n",
        "    }\n",
        "    headers = {'Content-Type': 'application/json'}\n",
        "    response = authed_session.post(listing_endpoint, data=json.dumps(payload), headers=headers)\n",
        "\n",
        "    if response.status_code in [200, 201]:\n",
        "      response_json = response.json()\n",
        "      linked_dataset_name = response_json.get('name', 'N/A')\n",
        "      print(f\"Successfully subscribed to listing! Linked dataset name: {project_id}.{linked_dataset_id}\")\n",
        "    elif response.status_code == 409:\n",
        "        print(f\"Error 409: Conflict. The dataset '{PROJECT_ID}.{LINKED_DATASET_ID}' might already be linked or exist.\")\n",
        "        print(\"Please check if the dataset already exists or if there's an existing subscription.\")\n",
        "        print(f\"Full error response: {response.json()}\")\n",
        "    else:\n",
        "        print(f\"An unexpected error occurred. Status Code: {response.status_code}\")\n",
        "        print(f\"Full error response: {response.json()}\")\n",
        "\n",
        "    return response\n",
        "\n",
        "\n",
        "# Create a linked dataset to the\n",
        "response = subscribe_to_listing(LISTING_ENDPOINT, YOUR_PROJECT_ID, LINKED_DATASET_ID, REGION)"
      ],
      "metadata": {
        "id": "FiuJ-1bXqXv1"
      },
      "id": "FiuJ-1bXqXv1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ],
      "metadata": {
        "id": "zHkJOKAkvTOG"
      },
      "id": "zHkJOKAkvTOG"
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_bq_jobs(\n",
        "    job_id_1: str,\n",
        "    job_id_2: str,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves and compares the statistics for two completed BigQuery jobs.\n",
        "\n",
        "    Args:\n",
        "     job_id_1: The job ID (e.g., 'bquxjob_...') for the first job.\n",
        "     job_id_2: The job ID for the second job.\n",
        "     project_id: (optional) The GCP project ID where the jobs were run,\n",
        "         defaults to the PROJECT_ID variable.\n",
        "     location: (optional) The location/region of the jobs (e.g., 'US', 'EU'),\n",
        "         defaults to the REGION variable.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        # By default, this uses Application Default Credentials\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Get job information\n",
        "        job_1 = client.get_job(job_id_1, project=project_id, location=location)\n",
        "        job_2 = client.get_job(job_id_2, project=project_id, location=location)\n",
        "\n",
        "        # Validate that they are query jobs\n",
        "        if not isinstance(job_1, bigquery.QueryJob) or not isinstance(\n",
        "            job_2, bigquery.QueryJob\n",
        "        ):\n",
        "            print(\n",
        "                \"Error: One or both job IDs do not correspond to a QueryJob.\",\n",
        "                file=sys.stderr,\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # Validate that they are complete\n",
        "        if job_1.state != \"DONE\" or job_2.state != \"DONE\":\n",
        "            print(\n",
        "                \"Error: One or both jobs are not in the 'DONE' state.\", file=sys.stderr\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if job_1.error_result or job_2.error_result:\n",
        "            print(\"Warning: One or both jobs completed with errors.\", file=sys.stderr)\n",
        "\n",
        "        # Extract statistics\n",
        "        stats_1 = {\n",
        "            \"id\": job_1.job_id,\n",
        "            \"rows_returned\": job_1.result().total_rows,\n",
        "            \"query_time\": (\n",
        "                (job_1.ended - job_1.started).total_seconds()\n",
        "                if job_1.ended and job_1.started\n",
        "                else 0\n",
        "            ),\n",
        "            \"bytes_scanned\": (\n",
        "                job_1.total_bytes_processed\n",
        "                if job_1.total_bytes_processed is not None\n",
        "                else 0\n",
        "            ),\n",
        "            \"slot_ms\": job_1.slot_millis if job_1.slot_millis is not None else 0,\n",
        "        }\n",
        "\n",
        "        stats_2 = {\n",
        "            \"id\": job_2.job_id,\n",
        "            \"rows_returned\": job_2.result().total_rows,\n",
        "            \"query_time\": (\n",
        "                (job_2.ended - job_2.started).total_seconds()\n",
        "                if job_2.ended and job_2.started\n",
        "                else 0\n",
        "            ),\n",
        "            \"bytes_scanned\": (\n",
        "                job_2.total_bytes_processed\n",
        "                if job_2.total_bytes_processed is not None\n",
        "                else 0\n",
        "            ),\n",
        "            \"slot_ms\": job_2.slot_millis if job_2.slot_millis is not None else 0,\n",
        "        }\n",
        "\n",
        "        # Print the comparison table\n",
        "        print_comparison_table(stats_1, stats_2)\n",
        "\n",
        "    except NotFound:\n",
        "        print(\n",
        "            \"Error: Job not found. Check the job IDs, project, and location.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\", file=sys.stderr)\n",
        "        print(\n",
        "            \"Please ensure you are authenticated ('gcloud auth application-default login') \"\n",
        "            \"and the job/project/location details are correct.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "def compare_bq_jobs_static(\n",
        "    job_1_dict: dict,\n",
        "    job_id_2: str,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves and compares the statistics for two completed BigQuery jobs.\n",
        "    The first argument is not a job_id to be retrieved from BQ, but instead,\n",
        "    it's a static dictionary with the job details.\n",
        "\n",
        "    Use this method when you want to compare a job against a static baseline.\n",
        "\n",
        "    Args:\n",
        "     job_1: A dictionary with the job 1 execution details.\n",
        "     job_id_2: The job ID for the second job.\n",
        "     project_id: (optional) The GCP project ID where the jobs were run,\n",
        "         defaults to the PROJECT_ID variable.\n",
        "     location: (optional) The location/region of the jobs (e.g., 'US', 'EU'),\n",
        "         defaults to the REGION variable.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Initialize the client\n",
        "        # By default, this uses Application Default Credentials\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Get job information\n",
        "        job_2 = client.get_job(job_id_2, project=project_id, location=location)\n",
        "\n",
        "        # Validate that they are query jobs\n",
        "        if not isinstance(\n",
        "            job_2, bigquery.QueryJob\n",
        "        ):\n",
        "            print(\n",
        "                \"Error: One or both job IDs do not correspond to a QueryJob.\",\n",
        "                file=sys.stderr,\n",
        "            )\n",
        "            return\n",
        "\n",
        "        # Validate that they are complete\n",
        "        if job_2.state != \"DONE\":\n",
        "            print(\n",
        "                \"Error: One or both jobs are not in the 'DONE' state.\", file=sys.stderr\n",
        "            )\n",
        "            return\n",
        "\n",
        "        if job_2.error_result:\n",
        "            print(\"Warning: One or both jobs completed with errors.\", file=sys.stderr)\n",
        "\n",
        "        # Extract statistics\n",
        "        stats_2 = {\n",
        "            \"id\": job_2.job_id,\n",
        "            \"rows_returned\": job_2.result().total_rows,\n",
        "            \"query_time\": (\n",
        "                (job_2.ended - job_2.started).total_seconds()\n",
        "                if job_2.ended and job_2.started\n",
        "                else 0\n",
        "            ),\n",
        "            \"bytes_scanned\": (\n",
        "                job_2.total_bytes_processed\n",
        "                if job_2.total_bytes_processed is not None\n",
        "                else 0\n",
        "            ),\n",
        "            \"slot_ms\": job_2.slot_millis if job_2.slot_millis is not None else 0,\n",
        "        }\n",
        "\n",
        "        # Print the comparison table\n",
        "        print_comparison_table(job_1_dict, stats_2)\n",
        "\n",
        "    except NotFound:\n",
        "        print(\n",
        "            \"Error: Job not found. Check the job IDs, project, and location.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\", file=sys.stderr)\n",
        "        print(\n",
        "            \"Please ensure you are authenticated ('gcloud auth application-default login') \"\n",
        "            \"and the job/project/location details are correct.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "def print_comparison_table(stats_1: dict, stats_2: dict):\n",
        "    \"\"\"Helper function to format and print an ASCII table comparing two BQ jobs.\"\"\"\n",
        "\n",
        "    # --- Helper functions for formatting ---\n",
        "    def format_bytes(b: int) -> str:\n",
        "        return humanize.naturalsize(b, binary=True)  # e.g., 1.2 GiB\n",
        "\n",
        "    def format_time(s: float) -> str:\n",
        "        return f\"{s:,.2f} s\"\n",
        "\n",
        "    def format_slot_ms(ms: int) -> str:\n",
        "        return f\"{ms:,.0f} ms\"\n",
        "\n",
        "    def format_rows(r: int) -> str:\n",
        "        if r is None:\n",
        "            return \"N/A\"\n",
        "        return f\"{r:,.0f}\"\n",
        "\n",
        "    def get_comparison(val_1, val_2) -> str:\n",
        "        \"\"\"Calculates the percentage change between val_1 and val_2.\"\"\"\n",
        "        if val_1 is None and val_2 is None:\n",
        "            return \"N/A\"\n",
        "        if val_1 is None:  # val_2 must be not None\n",
        "            return \"(+inf %)\" if val_2 > 0 else \"(No change)\"\n",
        "        if val_2 is None:  # val_1 must be not None\n",
        "            return \"(-100.0%)\" if val_1 > 0 else \"(No change)\"\n",
        "\n",
        "        # From here, val_1 and val_2 are both not None (are numbers)\n",
        "        if val_1 == 0:\n",
        "            if val_2 > 0:\n",
        "                return \"(+inf %)\"\n",
        "            if val_2 < 0:\n",
        "                return \"(-inf %)\"  # Should not happen for rows/bytes/time\n",
        "            return \"(No change)\"  # val_1 == 0 and val_2 == 0\n",
        "\n",
        "        percent = (val_2 - val_1) / val_1\n",
        "\n",
        "        if percent == 0:\n",
        "            return \"(No change)\"\n",
        "\n",
        "        # For time, bytes, and slot_ms, lower is generally better\n",
        "        comparison_text = \"slower/more\" if percent > 0 else \"faster/less\"\n",
        "        return f\"({percent:+.1%} {comparison_text})\"\n",
        "\n",
        "    # --- Prepare data for table ---\n",
        "    data = [\n",
        "        (\n",
        "            \"Query Time\",\n",
        "            stats_1[\"query_time\"],\n",
        "            stats_2[\"query_time\"],\n",
        "            format_time,\n",
        "            get_comparison,\n",
        "        ),\n",
        "        (\n",
        "            \"Bytes Scanned\",\n",
        "            stats_1[\"bytes_scanned\"],\n",
        "            stats_2[\"bytes_scanned\"],\n",
        "            format_bytes,\n",
        "            get_comparison,\n",
        "        ),\n",
        "        (\n",
        "            \"Slot Milliseconds\",\n",
        "            stats_1[\"slot_ms\"],\n",
        "            stats_2[\"slot_ms\"],\n",
        "            format_slot_ms,\n",
        "            get_comparison,\n",
        "        ),\n",
        "        (\n",
        "            \"Rows Returned\",\n",
        "            stats_1[\"rows_returned\"],\n",
        "            stats_2[\"rows_returned\"],\n",
        "            format_rows,\n",
        "            get_comparison,\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    # --- Calculate column widths ---\n",
        "    header_1 = f\"Job 1 ({stats_1['id'][-12:]})\"  # Show last 12 chars of ID\n",
        "    header_2 = f\"Job 2 ({stats_2['id'][-12:]})\"\n",
        "\n",
        "    formatted_job_1 = [row[3](row[1]) for row in data]\n",
        "    formatted_job_2 = [row[3](row[2]) for row in data]\n",
        "    formatted_comp = [row[4](row[1], row[2]) for row in data]\n",
        "\n",
        "    col_1_width = max(len(row[0]) for row in data) + 2  # Metric\n",
        "    col_2_width = max(max(len(s) for s in formatted_job_1), len(header_1)) + 2\n",
        "    col_3_width = max(max(len(s) for s in formatted_job_2), len(header_2)) + 2\n",
        "    col_4_width = max(max(len(s) for s in formatted_comp), len(\"Comparison\")) + 2\n",
        "\n",
        "    # --- Print Table ---\n",
        "    # The total width for the title row needs to account for the\n",
        "    # 3 extra '|' characters that separate the columns.\n",
        "    total_width = col_1_width + col_2_width + col_3_width + col_4_width + 3\n",
        "\n",
        "    def print_row(c1, c2, c3, c4, align_c1=\"left\", pad_char=\" \"):\n",
        "        c1_padded = (\n",
        "            c1.ljust(col_1_width, pad_char)\n",
        "            if align_c1 == \"left\"\n",
        "            else c1.rjust(col_1_width, pad_char)\n",
        "        )\n",
        "        c2_padded = c2.rjust(col_2_width, pad_char)  # Right-align numbers\n",
        "        c3_padded = c3.rjust(col_3_width, pad_char)\n",
        "        c4_padded = c4.rjust(col_4_width, pad_char)\n",
        "        print(f\"|{c1_padded}|{c2_padded}|{c3_padded}|{c4_padded}|\")\n",
        "\n",
        "    def print_divider(char=\"=\"):\n",
        "        print(\n",
        "            f\"+{char * col_1_width}+{char * col_2_width}+{char * col_3_width}+{char * col_4_width}+\"\n",
        "        )\n",
        "\n",
        "    print(\"\\n\" + \"+\" + \"=\" * total_width + \"+\")\n",
        "    title = \"BigQuery Job Comparison\"\n",
        "    print(f\"|{title:^{total_width}}|\")\n",
        "    print_divider()\n",
        "\n",
        "    # Headers\n",
        "    print_row(f\" Metric \", f\" {header_1} \", f\" {header_2} \", f\" Comparison \")\n",
        "    print_divider(\"-\")\n",
        "\n",
        "    # Data Rows\n",
        "    for i, row in enumerate(data):\n",
        "        metric_name = f\" {row[0]} \"\n",
        "        val_1_str = f\" {formatted_job_1[i]} \"\n",
        "        val_2_str = f\" {formatted_job_2[i]} \"\n",
        "        comp_str = f\" {formatted_comp[i]} \"\n",
        "        print_row(metric_name, val_1_str, val_2_str, comp_str)\n",
        "\n",
        "    print_divider(\"-\")\n",
        "    print()  # Newline at the end\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "def get_job_details_infoschema(\n",
        "    job_id: str = None, client: bigquery.Client = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Fetches details of a BigQuery job from INFORMATION_SCHEMA.JOBS and prints\n",
        "    it as a dataframe.\n",
        "\n",
        "    Args:\n",
        "     job_id: The job ID (e.g., 'bquxjob_...')\n",
        "     client: (optional) The bigquery.Client object to run the INFORMATION_SCHEMA query\n",
        "    \"\"\"\n",
        "    if client == None:\n",
        "        client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "    qs_table = f\"region-{REGION}.INFORMATION_SCHEMA.JOBS\"\n",
        "    sql = f\"SELECT * FROM `{qs_table}`\"\n",
        "\n",
        "    if job_id:\n",
        "        sql += f\" WHERE job_id = '{job_id}'\"\n",
        "    sql += f\" AND project_id = '{PROJECT_ID}'\"\n",
        "\n",
        "    print(sql)\n",
        "\n",
        "    df_result = client.query(sql).to_dataframe()\n",
        "    print(df_result)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "def run_bq_query(\n",
        "    query_sql: str,\n",
        "    cache: bool = False,\n",
        "    show_query_results: bool = False,\n",
        "    project_id: str = PROJECT_ID,\n",
        "    location: str = REGION,\n",
        ") -> tuple [str | None, dict | None]:\n",
        "    \"\"\"\n",
        "    Runs a BigQuery query, polls for completion, and prints job stats.\n",
        "\n",
        "    Args:\n",
        "     query_sql: The SQL query string to execute.\n",
        "     cache: (optional) Whether to use BigQuery caching. Default: False\n",
        "     show_query_results: If True, prints the first 10 rows of the result.\n",
        "     project_id: (optional) The GCP project ID where the jobs were run, defaults to the PROJECT_ID variable.\n",
        "     location: (optional) The location/region of the jobs (e.g., 'US', 'EU'), defaults to the REGION variable.\n",
        "\n",
        "    Returns:\n",
        "     A tuple containing:\n",
        "     - The job ID as a string if the job started, otherwise None.\n",
        "     - A dictionary containing job metadata if the job completed successfully, otherwise None.\n",
        "    \"\"\"\n",
        "    print(f\"Running query in project '{project_id}', location '{location}'...\")\n",
        "    try:\n",
        "        client = bigquery.Client(project=project_id)\n",
        "\n",
        "        # Start the query job\n",
        "        job_config = bigquery.QueryJobConfig(\n",
        "            priority=bigquery.QueryPriority.INTERACTIVE, use_query_cache=cache\n",
        "        )\n",
        "\n",
        "        job = client.query(query_sql, location=location, job_config=job_config)\n",
        "        print(f\"Job started. ID: {job.job_id}\")\n",
        "\n",
        "        job_start_time = time.time()\n",
        "        # Wait for the job to complete, polling every 5 seconds\n",
        "        while job.state in (\"RUNNING\", \"PENDING\"):\n",
        "            elapsed = time.time() - job_start_time\n",
        "            # \\r moves the cursor to the start of the line for an overwriting update\n",
        "            print(\n",
        "                f\" Job status: {job.state.lower()}... (elapsed: {elapsed:.0f}s) \",\n",
        "                end=\"\\r\",\n",
        "            )\n",
        "            time.sleep(5)\n",
        "            job.reload()\n",
        "\n",
        "        # Clear the \"running\" line\n",
        "        print(\" \" * 60, end=\"\\r\")\n",
        "\n",
        "        # Print the final job summary\n",
        "        print(f\"Job {job.job_id} finished with state: {job.state}\")\n",
        "\n",
        "        if job.error_result:\n",
        "            print(\"Error Details:\", file=sys.stderr)\n",
        "            print(job.error_result, file=sys.stderr)\n",
        "            return job.job_id, None\n",
        "\n",
        "        # Display job statistics if successful\n",
        "        if job.state == \"DONE\":\n",
        "            duration = (job.ended - job.started).total_seconds()\n",
        "            bytes_processed_val = (\n",
        "                job.total_bytes_processed\n",
        "                if job.total_bytes_processed is not None\n",
        "                else 0\n",
        "            )\n",
        "            bytes_scanned_formatted = humanize.naturalsize(\n",
        "                bytes_processed_val, binary=True\n",
        "            )\n",
        "            slot_ms_val = job.slot_millis if job.slot_millis is not None else 0\n",
        "\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "            job_result = None # Initialize\n",
        "            rows_returned = None\n",
        "\n",
        "            if job.num_dml_affected_rows is not None:\n",
        "                print(f\" Rows Affected (DML): {job.num_dml_affected_rows:,}\")\n",
        "            else:\n",
        "                # For SELECT queries, get total_rows from the result.\n",
        "                # This will be 0 for DDL statements.\n",
        "                try:\n",
        "                    # Calling .result() waits for the job to complete and gets the RowIterator\n",
        "                    job_result = job.result() # Get the result iterator\n",
        "                    rows_returned = job_result.total_rows\n",
        "                    print(f\" Rows Returned:    {rows_returned:,}\")\n",
        "                except Exception as e:\n",
        "                    # Handle cases like DDL where result() might not have total_rows\n",
        "                    print(f\" Rows Returned:    N/A (Not a SELECT query?)\")\n",
        "\n",
        "\n",
        "            # creating result dictionary\n",
        "            job_metadata_result = {'job_id': job.job_id, 'duration': duration, 'bytes_scanned': bytes_scanned_formatted, 'slot_time': slot_ms_val, 'cache_hit': job.cache_hit}\n",
        "\n",
        "\n",
        "            print(f\" Duration:      {duration:,.2f} s\")\n",
        "            print(\n",
        "                f\" Bytes Scanned:    {bytes_scanned_formatted} ({bytes_processed_val:,} bytes)\"\n",
        "            )\n",
        "            print(f\" Slot Time:      {slot_ms_val:,.0f} ms\")\n",
        "            print(\"-\" * 30)\n",
        "            print()  # newline\n",
        "\n",
        "            # Show query results\n",
        "            if show_query_results:\n",
        "                if job_result and rows_returned is not None and rows_returned > 0:\n",
        "                    print(\"Query Results (first 10 rows):\")\n",
        "                    print(\"-\" * 30)\n",
        "\n",
        "                    try:\n",
        "                        # Requires `pip install pandas db-dtypes`\n",
        "                        dataframe = job_result.to_dataframe()\n",
        "                        print(dataframe.head(10))\n",
        "                        print(\"-\" * 30)\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error printing query results with pandas: {e}\", file=sys.stderr)\n",
        "                        print(\"Please ensure 'pandas' and 'db-dtypes' libraries are installed.\", file=sys.stderr)\n",
        "\n",
        "\n",
        "        return job.job_id, job_metadata_result\n",
        "\n",
        "    except NotFound:\n",
        "        print(f\"Error: Project or location may be incorrect.\", file=sys.stderr)\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\", file=sys.stderr)\n",
        "        print(\n",
        "            \"Please ensure you are authenticated ('gcloud auth application-default login') \"\n",
        "            \"and the project/location details are correct.\",\n",
        "            file=sys.stderr,\n",
        "        )\n",
        "        return None, None"
      ],
      "metadata": {
        "id": "VOII_5xJphfC"
      },
      "id": "VOII_5xJphfC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the compare_bq_jobs function\n",
        "sql = \"\"\"select 1\"\"\"\n",
        "\n",
        "print(\"--- Running Job 1 ---\")\n",
        "job1_result = run_bq_query(sql)\n",
        "job_id_1 = job1_result[0] if job1_result else None\n",
        "\n",
        "print(\"\\n--- Running Job 2 ---\")\n",
        "job2_result = run_bq_query(sql)\n",
        "job_id_2 = job2_result[0] if job2_result else None\n",
        "\n",
        "print(\"\\n--- Comparing Jobs ---\")\n",
        "if job_id_1 and job_id_2:\n",
        "    compare_bq_jobs(job_id_1, job_id_2)\n",
        "else:\n",
        "    print(\"Comparison skipped: One or both BQ jobs failed to return a valid job ID.\")"
      ],
      "metadata": {
        "id": "H0LL6KvQpvY8"
      },
      "id": "H0LL6KvQpvY8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additional Useful Information"
      ],
      "metadata": {
        "id": "d9SDCYvSo9Sa"
      },
      "id": "d9SDCYvSo9Sa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to see the ER Model diagram for these tables, clik [here](https://datacadamia.com/data/type/relation/benchmark/tpcds/schema).\n",
        "\n",
        "Please find the exact DDL for the tables involved in this query below.\n",
        "\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.store_sales`\n",
        "(\n",
        "  ss_sold_date_sk INT64,\n",
        "  ss_sold_time_sk INT64,\n",
        "  ss_item_sk INT64,\n",
        "  ss_customer_sk INT64,\n",
        "  ss_cdemo_sk INT64,\n",
        "  ss_hdemo_sk INT64,\n",
        "  ss_addr_sk INT64,\n",
        "  ss_store_sk INT64,\n",
        "  ss_promo_sk INT64,\n",
        "  ss_ticket_number INT64,\n",
        "  ss_quantity INT64,\n",
        "  ss_wholesale_cost FLOAT64,\n",
        "  ss_list_price FLOAT64,\n",
        "  ss_sales_price FLOAT64,\n",
        "  ss_ext_discount_amt FLOAT64,\n",
        "  ss_ext_sales_price FLOAT64,\n",
        "  ss_ext_wholesale_cost FLOAT64,\n",
        "  ss_ext_list_price FLOAT64,\n",
        "  ss_ext_tax FLOAT64,\n",
        "  ss_coupon_amt FLOAT64,\n",
        "  ss_net_paid FLOAT64,\n",
        "  ss_net_paid_inc_tax FLOAT64,\n",
        "  ss_net_profit FLOAT64\n",
        ");\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.customer_address`\n",
        "(\n",
        "  ca_address_sk INT64,\n",
        "  ca_address_id STRING,\n",
        "  ca_street_number STRING,\n",
        "  ca_street_name STRING,\n",
        "  ca_street_type STRING,\n",
        "  ca_suite_number STRING,\n",
        "  ca_city STRING,\n",
        "  ca_county STRING,\n",
        "  ca_state STRING,\n",
        "  ca_zip STRING,\n",
        "  ca_country STRING,\n",
        "  ca_gmt_offset FLOAT64,\n",
        "  ca_location_type STRING\n",
        ");\n",
        "```\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.household_demographics`\n",
        "(\n",
        "  hd_demo_sk INT64,\n",
        "  hd_income_band_sk INT64,\n",
        "  hd_buy_potential STRING,\n",
        "  hd_dep_count INT64,\n",
        "  hd_vehicle_count INT64\n",
        ");\n",
        "```\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.customer`\n",
        "(\n",
        "  c_customer_sk INT64,\n",
        "  c_customer_id STRING,\n",
        "  c_current_cdemo_sk INT64,\n",
        "  c_current_hdemo_sk INT64,\n",
        "  c_current_addr_sk INT64,\n",
        "  c_first_shipto_date_sk INT64,\n",
        "  c_first_sales_date_sk INT64,\n",
        "  c_salutation STRING,\n",
        "  c_first_name STRING,\n",
        "  c_last_name STRING,\n",
        "  c_preferred_cust_flag STRING,\n",
        "  c_birth_day INT64,\n",
        "  c_birth_month INT64,\n",
        "  c_birth_year INT64,\n",
        "  c_birth_country STRING,\n",
        "  c_login STRING,\n",
        "  c_email_address STRING,\n",
        "  c_last_review_date_sk INT64\n",
        ");\n",
        "```\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.date_dim`\n",
        "(\n",
        "  d_date_sk INT64,\n",
        "  d_date_id STRING,\n",
        "  d_date DATE,\n",
        "  d_month_seq INT64,\n",
        "  d_week_seq INT64,\n",
        "  d_quarter_seq INT64,\n",
        "  d_year INT64,\n",
        "  d_dow INT64,\n",
        "  d_moy INT64,\n",
        "  d_dom INT64,\n",
        "  d_qoy INT64,\n",
        "  d_fy_year INT64,\n",
        "  d_fy_quarter_seq INT64,\n",
        "  d_fy_week_seq INT64,\n",
        "  d_day_name STRING,\n",
        "  d_quarter_name STRING,\n",
        "  d_holiday STRING,\n",
        "  d_weekend STRING,\n",
        "  d_following_holiday STRING,\n",
        "  d_first_dom INT64,\n",
        "  d_last_dom INT64,\n",
        "  d_same_day_ly INT64,\n",
        "  d_same_day_lq INT64,\n",
        "  d_current_day STRING,\n",
        "  d_current_week STRING,\n",
        "  d_current_month STRING,\n",
        "  d_current_quarter STRING,\n",
        "  d_current_year STRING\n",
        ");\n",
        "```\n",
        "\n",
        "```\n",
        "CREATE TABLE `<PROJECT>.<DATASET>.store`\n",
        "(\n",
        "  s_store_sk INT64,\n",
        "  s_store_id STRING,\n",
        "  s_rec_start_date DATE,\n",
        "  s_rec_end_date DATE,\n",
        "  s_closed_date_sk INT64,\n",
        "  s_store_name STRING,\n",
        "  s_number_employees INT64,\n",
        "  s_floor_space INT64,\n",
        "  s_hours STRING,\n",
        "  s_manager STRING,\n",
        "  s_market_id INT64,\n",
        "  s_geography_class STRING,\n",
        "  s_market_desc STRING,\n",
        "  s_market_manager STRING,\n",
        "  s_division_id INT64,\n",
        "  s_division_name STRING,\n",
        "  s_company_id INT64,\n",
        "  s_company_name STRING,\n",
        "  s_street_number STRING,\n",
        "  s_street_name STRING,\n",
        "  s_street_type STRING,\n",
        "  s_suite_number STRING,\n",
        "  s_city STRING,\n",
        "  s_county STRING,\n",
        "  s_state STRING,\n",
        "  s_zip STRING,\n",
        "  s_country STRING,\n",
        "  s_gmt_offset FLOAT64,\n",
        "  s_tax_percentage FLOAT64\n",
        ");\n",
        "```"
      ],
      "metadata": {
        "id": "dBMrCX6zpB9h"
      },
      "id": "dBMrCX6zpB9h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery Optimization Techniques (15 mins)\n",
        "### BQ SME Academy 2025\n",
        "---\n",
        "### 🎯 Your Goal\n",
        "\n",
        "For each exercise, your goal is to run the queries and analyze the **\"BigQuery Job Comparison\"** table in the output. Pay close attention to these three key metrics:\n",
        "\n",
        "* **Bytes Scanned:** The amount of data read from disk. This is the primary driver of query **cost**.\n",
        "* **Slot Milliseconds:** The total compute time consumed by the query. This is a measure of query **complexity and speed**.\n",
        "* **Query Time:** The wall-clock time it took for the query to complete.\n",
        "\n",
        "The objective is to understand **why** these metrics change between the un-optimized and optimized queries.\n",
        "\n",
        "### 💿 The Dataset\n",
        "\n",
        "We will be using a 10TB version of the TPC-DS benchmark dataset\n",
        "\n",
        "### 📝 How to use this section\n",
        "\n",
        "1.  **Follow the Exercises:** Proceed through the numbered exercises in order.\n",
        "2.  **Read the Explanation:** Before each code cell, read the markdown explanation of the anti-pattern and the proposed fix.\n",
        "3.  **Execute the Queries:** Run the code cell for each exercise to see the performance comparison.\n",
        "4.  **Analyze the Results:** Review the output table and make sure you understand why the metrics changed."
      ],
      "metadata": {
        "id": "lc1WEFWg171V"
      },
      "id": "lc1WEFWg171V"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup -----\n",
        "LAB01_DATASET_ID = LINKED_DATASET_ID\n"
      ],
      "metadata": {
        "id": "cM1ITRNT171W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "cM1ITRNT171W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) Enabling the Advanced Query Runtime\n",
        "\n",
        "BigQuery's **advanced query runtime** is a project-level feature. It is designed to accelerate complex analytical queries, especially those with large joins, aggregations, and window functions, by using more dynamic resource management during execution.\n",
        "\n",
        "For this lab, you can enable it to see if it provides an additional performance boost to the complex queries. This is a one-time setting for your project in a specific region.\n",
        "\n",
        "#### How to Enable\n",
        "\n",
        "You can enable the advanced runtime by executing the following SQL command.\n",
        "\n",
        "**Note:** You must run this command in the specific region where you want the setting to apply\n",
        "\n",
        "```python\n",
        "# --- Enable Advanced Query Runtime ---\n",
        "\n",
        "sql_enable_advanced_runtime = f\"\"\"\n",
        "ALTER PROJECT `{PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        "  `region-{REGION}.query_runtime` = \"advanced\"\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# To enable, uncomment and run the line below\n",
        "# run_bq_query(sql_enable_advanced_runtime, location=LOCATION)\n",
        "\n",
        "print(\"Advanced runtime enabled. You can now proceed with the lab exercises.\")\n",
        "\n",
        "```\n",
        "\n",
        "#### How to Verify the Setting\n",
        "\n",
        "You can check the current runtime setting for your project by running the following query in the appropriate region:\n",
        "```sql\n",
        "SELECT *\n",
        "FROM `region-{REGION}`.INFORMATION_SCHEMA.PROJECT_OPTIONS\n",
        "WHERE option_name = 'query_runtime';\n",
        "```"
      ],
      "metadata": {
        "id": "xSeKUQ8P171W"
      },
      "id": "xSeKUQ8P171W"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Enable Advanced Query Runtime ---\n",
        "\n",
        "sql_enable_advanced_runtime = f\"\"\"\n",
        "ALTER PROJECT `{YOUR_PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        "  `region-US.query_runtime` = \"advanced\"\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_enable_advanced_runtime, location=REGION)\n",
        "\n",
        "print(\"Advanced runtime enabled. You can now proceed with the lab exercises.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "3blR0IcF171W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "3blR0IcF171W"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Disable Advanced Query Runtime ---\n",
        "\n",
        "sql_disable_advanced_runtime = f\"\"\"\n",
        "ALTER PROJECT `{YOUR_PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        "  `region-US.query_runtime` = NULL\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# To disable, uncomment and run the line below\n",
        "# run_bq_query(sql_disable_advanced_runtime, location=REGION)\n",
        "\n",
        "#print(\"Command to disable advanced runtime is ready to be executed.\")"
      ],
      "metadata": {
        "id": "HsN3jy6n171X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HsN3jy6n171X"
    },
    {
      "cell_type": "code",
      "source": [
        "sql_verify_runtime = f\"\"\"\n",
        "SELECT option_name, option_value\n",
        "FROM `region-US`.INFORMATION_SCHEMA.PROJECT_OPTIONS\n",
        "WHERE option_name = 'query_runtime';\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql_verify_runtime, location=REGION, show_query_results=True)"
      ],
      "metadata": {
        "id": "dmuTO_mz171X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "dmuTO_mz171X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.  Selecting Unnecessary Columns (`SELECT *`)\n",
        "\n",
        "**The Anti-Pattern:** Using `SELECT *` is one of the most common and costly mistakes in BigQuery. Because BigQuery is a columnar database, it charges based on the amount of data read from the columns you query. `SELECT *` forces BigQuery to read all data from every single column in the table, even if you only need a few.\n",
        "\n",
        "**The Fix:** The solution is simple but highly effective: only select the specific columns you need for your analysis.\n",
        "\n",
        "**Expected Outcome:** By selecting only the necessary columns, you dramatically reduce the amount of data processed. You should see a massive reduction in **Bytes Scanned**, which directly translates to lower query costs."
      ],
      "metadata": {
        "id": "5A12-viE171X"
      },
      "id": "5A12-viE171X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "7Zx7oSp9171X"
      },
      "source": [
        "# --- 1. SELECT * vs. SELECT specific_columns ---\n",
        "\n",
        "# Note: We add a filter on a non-clustered field and a LIMIT to make the query runnable.\n",
        "# The key is to observe the difference in \"Bytes Scanned\".\n",
        "\n",
        "a1_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Scans all 23 columns from the table.\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "WHERE\n",
        "  ss_ticket_number = 46153516\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "a1_opt_query = f\"\"\"\n",
        "-- Optimized: Scans only the 3 columns needed.\n",
        "SELECT\n",
        "  ss_sold_date_sk,\n",
        "  ss_item_sk,\n",
        "  ss_customer_sk\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "WHERE\n",
        "  ss_ticket_number = 46153516\n",
        "LIMIT 10;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (SELECT *) ---\")\n",
        "a1_job1 = run_bq_query(a1_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (SELECT specific_columns) ---\")\n",
        "a1_job2 = run_bq_query(a1_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a1_job1 and a1_job2:\n",
        "    compare_bq_jobs(a1_job1[0], a1_job2[0])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7Zx7oSp9171X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.  Filtering Early with Conditional Aggregation\n",
        "\n",
        "**The Anti-Pattern:** A common but highly inefficient pattern is to join a massive table and then use a `CASE` statement inside an aggregate function (like `SUM`) to compute a value for a specific subset of data. This forces BigQuery to process the entire massive join, evaluating the condition for every single row, which is incredibly wasteful.\n",
        "\n",
        "**The Fix:** Pre-filter and pre-aggregate the subset of data in a CTE first. This creates a very small, intermediate table. You can then `LEFT JOIN` this small table to get the result, which is orders of magnitude more efficient than processing the entire large table.\n",
        "\n",
        "**The Scenario:** For every customer, calculate their total lifetime spending and their total spending from just January 2001."
      ],
      "metadata": {
        "id": "AdLJMck_171X"
      },
      "id": "AdLJMck_171X"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "a2_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Joins the entire store_sales table to customer, then\n",
        "-- evaluates a CASE statement across billions of rows to find Jan 2001 sales.\n",
        "SELECT\n",
        "  c.c_customer_id,\n",
        "  SUM(s.ss_net_paid) AS total_lifetime_spend,\n",
        "  SUM(CASE\n",
        "      WHEN s.ss_sold_date_sk BETWEEN 2451911 AND 2451941 THEN s.ss_net_paid\n",
        "      ELSE 0\n",
        "    END) AS total_jan_2001_spend\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.customer` AS c\n",
        "LEFT JOIN\n",
        "  `{LAB01_DATASET_ID}.store_sales` AS s\n",
        "  ON c.c_customer_sk = s.ss_customer_sk\n",
        "GROUP BY\n",
        "  c.c_customer_id\n",
        "LIMIT 10000;\n",
        "\"\"\"\n",
        "\n",
        "a2_opt_query = f\"\"\"\n",
        "-- Optimized: Pre-aggregates the small slice of Jan 2001 sales first,\n",
        "-- then joins the small result back to the customer table.\n",
        "WITH JanSales AS (\n",
        "  SELECT\n",
        "    ss_customer_sk,\n",
        "    SUM(ss_net_paid) AS total_jan_spend\n",
        "  FROM\n",
        "    `{LAB01_DATASET_ID}.store_sales`\n",
        "  WHERE\n",
        "    ss_sold_date_sk BETWEEN 2451911 AND 2451941 -- Filter is applied EARLY\n",
        "  GROUP BY\n",
        "    ss_customer_sk\n",
        "),\n",
        "TotalSales AS (\n",
        "  -- Also pre-aggregating total sales for a more efficient final join\n",
        "  SELECT\n",
        "    ss_customer_sk,\n",
        "    SUM(ss_net_paid) as total_lifetime_spend\n",
        "  FROM\n",
        "    `{LAB01_DATASET_ID}.store_sales`\n",
        "  GROUP BY\n",
        "    ss_customer_sk\n",
        ")\n",
        "SELECT\n",
        "  c.c_customer_id,\n",
        "  ts.total_lifetime_spend,\n",
        "  js.total_jan_spend\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.customer` AS c\n",
        "LEFT JOIN\n",
        "  JanSales AS js\n",
        "  ON c.c_customer_sk = js.ss_customer_sk\n",
        "LEFT JOIN\n",
        "  TotalSales AS ts\n",
        "  ON c.c_customer_sk = ts.ss_customer_sk\n",
        "LIMIT 10000;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (Filtering Late) ---\")\n",
        "a2_job1 = run_bq_query(a2_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (Filtering Early) ---\")\n",
        "a2_job2 = run_bq_query(a2_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a2_job1 and a2_job2:\n",
        "    compare_bq_jobs(a2_job1[0], a2_job2[0])"
      ],
      "metadata": {
        "id": "E0fLLEYB171X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "E0fLLEYB171X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2.1. Trusting the Optimizer: Join Reordering\n",
        "\n",
        "**The Scenario:** A common pattern is to join a large fact table to smaller dimension tables to filter on an attribute (e.g., find all sales for customers in a specific state). An intuitive \"fix\" might be to pre-join and filter the small tables in a CTE before joining them to the large table.\n",
        "\n",
        "**The Lesson:** In this exercise, we will see if our manual optimization can beat BigQuery's built-in, cost-based query optimizer. The optimizer analyzes all tables in a query and automatically reorders the joins to be as efficient as possible.\n",
        "\n",
        "**Expected Outcome:** The optimizer is incredibly smart. It knows to join the small `customer` and `customer_address` tables first before touching the massive `store_sales` table, even in the \"un-optimized\" query. You will likely see very similar performance for both queries, proving that for join ordering, you can often trust the optimizer to find the best path."
      ],
      "metadata": {
        "id": "QbPeNJQz171X"
      },
      "id": "QbPeNJQz171X"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 2. Filtering Late vs. Filtering Early (Corrected Example) ---\n",
        "# Scenario: Find the total sales for all customers in Georgia ('GA').\n",
        "\n",
        "a2_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Joins three tables, including the massive 28.8 billion row\n",
        "-- store_sales table, before filtering for customers in Georgia.\n",
        "SELECT\n",
        "  SUM(s.ss_net_paid) as total_ga_sales\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales` AS s\n",
        "JOIN\n",
        "  `{LAB01_DATASET_ID}.customer` AS c\n",
        "  ON s.ss_customer_sk = c.c_customer_sk\n",
        "JOIN\n",
        "  `{LAB01_DATASET_ID}.customer_address` AS ca\n",
        "  ON c.c_current_addr_sk = ca.ca_address_sk -- The necessary join to get the state\n",
        "WHERE\n",
        "  ca.ca_state = 'GA'; -- Filter applied after the massive joins\n",
        "\"\"\"\n",
        "\n",
        "a2_opt_query = f\"\"\"\n",
        "-- Optimized: Finds the small list of Georgia customers FIRST by joining\n",
        "-- the two small dimension tables, then joins that tiny list to the sales table.\n",
        "WITH georgia_customers AS (\n",
        "  SELECT c.c_customer_sk\n",
        "  FROM\n",
        "    `{LAB01_DATASET_ID}.customer` AS c\n",
        "  JOIN\n",
        "    `{LAB01_DATASET_ID}.customer_address` AS ca\n",
        "    ON c.c_current_addr_sk = ca.ca_address_sk -- Join small tables first\n",
        "  WHERE\n",
        "    ca.ca_state = 'GA'\n",
        ")\n",
        "SELECT\n",
        "  SUM(s.ss_net_paid) as total_ga_sales\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales` AS s\n",
        "JOIN\n",
        "  georgia_customers AS gc\n",
        "  ON s.ss_customer_sk = gc.c_customer_sk;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (Filtering Late) ---\")\n",
        "a2_job1 = run_bq_query(a2_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (Filtering Early) ---\")\n",
        "a2_job2 = run_bq_query(a2_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a2_job1 and a2_job2:\n",
        "    compare_bq_jobs(a2_job1[0], a2_job2[0])"
      ],
      "metadata": {
        "id": "by9fZeRg171X"
      },
      "execution_count": null,
      "outputs": [],
      "id": "by9fZeRg171X"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Inefficient `ORDER BY`\n",
        "\n",
        "**The Anti-Pattern:** Running an `ORDER BY` on a massive table without a `LIMIT` clause is extremely resource-intensive. To produce a total ordering of the data, BigQuery must gather all the rows onto a single worker for a final sort. This operation is very slow and will often fail with a \"resources exceeded\" error.\n",
        "\n",
        "**The Fix:** Always pair `ORDER BY` with a `LIMIT` when working with large datasets. This allows BigQuery to perform a much more efficient, distributed \"top-N\" sort, where only the top results from each worker need to be gathered and sorted.\n",
        "\n",
        "**Expected Outcome:** The optimized query will complete much faster, showing a huge reduction in **Query Time** and **Slot Milliseconds**. The **Bytes Scanned** will likely be the same, as BigQuery still needs to read the full column to determine which values are the top ones."
      ],
      "metadata": {
        "id": "qQ9uiz9D171Y"
      },
      "id": "qQ9uiz9D171Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Inefficient ORDER BY ---\n",
        "\n",
        "# The un-optimized query below is designed to fail or take a very long time.\n",
        "# It demonstrates an operation that should not be run on a large dataset.\n",
        "# You may want to cancel it after observing its high slot usage.\n",
        "\n",
        "a3_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Attempts to sort the entire 28.8 billion row table.\n",
        "-- This will consume massive resources and likely fail.\n",
        "SELECT\n",
        "  ss_net_paid\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "ORDER BY\n",
        "  ss_net_paid DESC;\n",
        "\"\"\"\n",
        "\n",
        "a3_opt_query = f\"\"\"\n",
        "-- Optimized: Efficiently finds the top 100 sales using a LIMIT.\n",
        "-- This allows BigQuery to perform a distributed top-N sort.\n",
        "SELECT\n",
        "  ss_net_paid\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "ORDER BY\n",
        "  ss_net_paid DESC\n",
        "LIMIT 100;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (ORDER BY without LIMIT) ---\")\n",
        "a3_job1 = run_bq_query(a3_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (ORDER BY with LIMIT) ---\")\n",
        "a3_job2 = run_bq_query(a3_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a3_job1 and a3_job2:\n",
        "    compare_bq_jobs(a3_job1[0], a3_job2[0])"
      ],
      "metadata": {
        "id": "GHwfUXLt171Y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "GHwfUXLt171Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.  Accidental Many-to-Many Join\n",
        "\n",
        "**The Anti-Pattern:** This is a subtle but dangerous mistake. When you join multiple large \"fact\" tables on a common, non-unique key (like a date), you create a hidden Cartesian product. For each date, every web sale is joined with every store sale, leading to massively inflated, incorrect results and terrible performance.\n",
        "\n",
        "**The Fix:** The correct pattern is to **aggregate before you join**. First, calculate the daily totals for each fact table in separate CTEs. Then, join the much smaller, pre-aggregated results. This ensures the final join is a simple and efficient one-to-one lookup.\n",
        "\n",
        "**Expected Outcome:** The optimized query will be dramatically faster, showing a huge reduction in **Slot Milliseconds**. More importantly, it will produce the **correct, non-inflated results**."
      ],
      "metadata": {
        "id": "xWw3djM8171Y"
      },
      "id": "xWw3djM8171Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Accidental Many-to-Many Join ---\n",
        "# The business goal is to compare sales for a single store and web site for a single day: June 2nd, 2001\n",
        "\n",
        "a4_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Creates a many-to-many join between a single store's sales\n",
        "-- and a single web site's sales for one day.\n",
        "SELECT\n",
        "  d.d_date,\n",
        "  SUM(ws.ws_net_paid_inc_tax) AS inflated_web_sales,\n",
        "  SUM(ss.ss_net_paid_inc_tax) AS inflated_store_sales\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.date_dim` d\n",
        "JOIN\n",
        "  `{LAB01_DATASET_ID}.web_sales` ws\n",
        "  ON d.d_date_sk = ws.ws_sold_date_sk\n",
        "JOIN\n",
        "  `{LAB01_DATASET_ID}.store_sales` ss\n",
        "  ON d.d_date_sk = ss.ss_sold_date_sk\n",
        "WHERE\n",
        "  d.d_date = '2001-06-02'\n",
        "  AND ss.ss_store_sk = 25 -- Filter to a single store\n",
        "  AND ws.ws_web_site_sk = 5 -- Filter to a single web site\n",
        "GROUP BY\n",
        "  d.d_date;\n",
        "\"\"\"\n",
        "\n",
        "a4_opt_query = f\"\"\"\n",
        "-- Optimized: Aggregate the filtered data from each table *before* joining.\n",
        "WITH WebSales AS (\n",
        "  SELECT\n",
        "    ws_sold_date_sk,\n",
        "    SUM(ws_net_paid_inc_tax) AS web_sales_total\n",
        "  FROM\n",
        "    `{LAB01_DATASET_ID}.web_sales`\n",
        "  WHERE\n",
        "    ws_sold_date_sk = 2452063 -- Corresponds to 2001-06-02\n",
        "    AND ws_web_site_sk = 5 -- **CORRECTION: Was 'ws_ws_web_site_sk'**\n",
        "  GROUP BY\n",
        "    ws_sold_date_sk\n",
        "),\n",
        "StoreSales AS (\n",
        "  SELECT\n",
        "    ss_sold_date_sk,\n",
        "    SUM(ss_net_paid_inc_tax) AS store_sales_total\n",
        "  FROM\n",
        "    `{LAB01_DATASET_ID}.store_sales`\n",
        "  WHERE\n",
        "    ss_sold_date_sk = 2452063 -- Corresponds to 2001-06-02\n",
        "    AND ss_store_sk = 25 -- Filter to a single store\n",
        "  GROUP BY\n",
        "    ss_sold_date_sk\n",
        ")\n",
        "SELECT\n",
        "  d.d_date,\n",
        "  ws.web_sales_total,\n",
        "  ss.store_sales_total\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.date_dim` d\n",
        "LEFT JOIN\n",
        "  WebSales ws ON d.d_date_sk = ws.ws_sold_date_sk\n",
        "LEFT JOIN\n",
        "  StoreSales ss ON d.d_date_sk = ss.ss_sold_date_sk\n",
        "WHERE\n",
        "  d.d_date = '2001-06-02';\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (Scoped to Store and Web Site) ---\")\n",
        "a4_job1 = run_bq_query(a4_unopt_query, show_query_results=True)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (Scoped and Pre-Aggregated) ---\")\n",
        "a4_job2 = run_bq_query(a4_opt_query, show_query_results=True)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a4_job1 and a4_job2:\n",
        "    compare_bq_jobs(a4_job1[0], a4_job2[0])"
      ],
      "metadata": {
        "id": "Bn1uaGLm171Y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Bn1uaGLm171Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.  JavaScript UDF vs. Native SQL Function\n",
        "\n",
        "**The Anti-Pattern:** While User-Defined Functions (UDFs) are powerful, using them for simple tasks that a native function can handle is inefficient. There is significant overhead in starting the JavaScript engine and passing data back and forth between the SQL and JS environments for every single row.\n",
        "\n",
        "**The Fix:** Whenever a built-in SQL function exists for your task, use it. Native functions are written in C++ and are tightly integrated into the BigQuery engine, making them orders of magnitude faster.\n",
        "\n",
        "**Expected Outcome:** You will see a significant reduction in **Slot Milliseconds** and **Query Time**, demonstrating the raw performance advantage of native functions over UDFs for common tasks."
      ],
      "metadata": {
        "id": "OLQiOPrY171Y"
      },
      "id": "OLQiOPrY171Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. JavaScript UDF vs. Native SQL Function ---\n",
        "# Scenario: We need to format a date into a readable string like \"Monday, January 15, 2001\".\n",
        "# We will process all dates from the date_dim table for the year 2001.\n",
        "\n",
        "a5_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Using a JavaScript UDF for a simple date formatting task.\n",
        "CREATE TEMP FUNCTION formatDate(d DATE)\n",
        "RETURNS STRING\n",
        "LANGUAGE js AS r\\\"\\\"\\\"\n",
        "  const options = {{ weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' }};\n",
        "  return d.toLocaleDateString('en-US', options);\n",
        "\\\"\\\"\\\";\n",
        "\n",
        "SELECT\n",
        "  d_date_sk,\n",
        "  formatDate(d_date) AS formatted_date\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.date_dim`\n",
        "WHERE\n",
        "  d_year = 2001;\n",
        "\"\"\"\n",
        "\n",
        "a5_opt_query = f\"\"\"\n",
        "-- Optimized: Using the highly efficient, built-in FORMAT_DATE function.\n",
        "SELECT\n",
        "  d_date_sk,\n",
        "  FORMAT_DATE('%A, %B %d, %Y', d_date) AS formatted_date\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.date_dim`\n",
        "WHERE\n",
        "  d_year = 2001;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (JavaScript UDF) ---\")\n",
        "a5_job1 = run_bq_query(a5_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (Native SQL Function) ---\")\n",
        "a5_job2 = run_bq_query(a5_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a5_job1 and a5_job2:\n",
        "    compare_bq_jobs(a5_job1[0], a5_job2[0])"
      ],
      "metadata": {
        "id": "J4gIGE5-171Y"
      },
      "execution_count": null,
      "outputs": [],
      "id": "J4gIGE5-171Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.  Data Exploration: `LIMIT` vs. `TABLESAMPLE`\n",
        "\n",
        "**The Anti-Pattern:** Using `LIMIT` to get a quick preview of a large table is a common habit, but it can be inefficient. BigQuery may still need to perform a large scan to find and return the first 1,0000 rows, leading to higher-than-expected costs and delays.\n",
        "\n",
        "**The Fix:** For a statistically representative preview of your data, use `TABLESAMPLE SYSTEM`. This function is specifically designed for cheap and fast exploration, as it only reads a small, random percentage of the underlying data blocks.\n",
        "\n",
        "**Expected Outcome:** The `TABLESAMPLE` query will be dramatically faster and cheaper. You should see a massive reduction in both **Bytes Scanned** and **Query Time**."
      ],
      "metadata": {
        "id": "dqd_xb7R171Y"
      },
      "id": "dqd_xb7R171Y"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. LIMIT vs. TABLESAMPLE ---\n",
        "\n",
        "a6_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Using LIMIT for exploration can still scan a lot of data.\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "  WHERE RAND() < 0.0000005 -- Filter for a small random sample\n",
        "ORDER BY\n",
        "  RAND()\n",
        "LIMIT 10000;\n",
        "\"\"\"\n",
        "\n",
        "a6_opt_query = f\"\"\"\n",
        "-- Optimized: Using TABLESAMPLE is cheaper and faster for a random preview.\n",
        "-- For a 10TB table, even a tiny percentage is enough for a sample.\n",
        "SELECT\n",
        "  *\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales` TABLESAMPLE SYSTEM (0.00001 PERCENT);\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (LIMIT) ---\")\n",
        "a6_job1 = run_bq_query(a6_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (TABLESAMPLE) ---\")\n",
        "a6_job2 = run_bq_query(a6_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a6_job1 and a6_job2:\n",
        "    compare_bq_jobs(a6_job1[0], a6_job2[0])"
      ],
      "metadata": {
        "id": "mx5hZdPW171Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "mx5hZdPW171Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.  Exact vs. Approximate Aggregations\n",
        "\n",
        "**The Anti-Pattern:** Calculating an exact `COUNT(DISTINCT)` on a column with millions or billions of unique values (a \"high cardinality\" column) is computationally expensive. It requires significant resources to track every unique value encountered.\n",
        "\n",
        "**The Fix:** For use cases where a highly accurate estimate is sufficient (like dashboards or general analysis), use `APPROX_COUNT_DISTINCT`. This function uses the efficient HyperLogLog++ algorithm to provide an estimate with a very small margin of error, but with much lower computational cost.\n",
        "\n",
        "**Expected Outcome:** You will see a significant reduction in **Slot Milliseconds**, highlighting the performance benefits of choosing approximation when perfect precision isn't a strict requirement."
      ],
      "metadata": {
        "id": "pvw4rxAn171Z"
      },
      "id": "pvw4rxAn171Z"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 7. COUNT(DISTINCT) vs. APPROX_COUNT_DISTINCT ---\n",
        "# We are filtering to a single year (2001) to ensure the query\n",
        "# completes in a reasonable time for the lab.\n",
        "\n",
        "a7_unopt_query = f\"\"\"\n",
        "-- Un-Optimized: Exact distinct count on a high-cardinality column.\n",
        "SELECT\n",
        "  ss_store_sk,\n",
        "  COUNT(DISTINCT ss_ticket_number) AS exact_ticket_count\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "WHERE\n",
        "  ss_sold_date_sk BETWEEN 2451911 AND 2452275 -- Filter for year 2001\n",
        "GROUP BY\n",
        "  ss_store_sk\n",
        "ORDER BY\n",
        "  ss_store_sk;\n",
        "\"\"\"\n",
        "\n",
        "a7_opt_query = f\"\"\"\n",
        "-- Optimized: Approximate count is much more efficient.\n",
        "SELECT\n",
        "  ss_store_sk,\n",
        "  APPROX_COUNT_DISTINCT(ss_ticket_number) AS approximate_ticket_count\n",
        "FROM\n",
        "  `{LAB01_DATASET_ID}.store_sales`\n",
        "WHERE\n",
        "  ss_sold_date_sk BETWEEN 2451911 AND 2452275 -- Filter for year 2001\n",
        "GROUP BY\n",
        "  ss_store_sk\n",
        "ORDER BY\n",
        "  ss_store_sk;\n",
        "\"\"\"\n",
        "\n",
        "# Run the queries\n",
        "print(\"--- Running Un-Optimized Query (COUNT DISTINCT for 1 Year) ---\")\n",
        "a7_job1 = run_bq_query(a7_unopt_query)\n",
        "\n",
        "print(\"\\n--- Running Optimized Query (APPROX_COUNT_DISTINCT for 1 Year) ---\")\n",
        "a7_job2 = run_bq_query(a7_opt_query)\n",
        "\n",
        "# Show both jobs side by side\n",
        "if a7_job1 and a7_job2:\n",
        "    compare_bq_jobs(a7_job1[0], a7_job2[0])"
      ],
      "metadata": {
        "id": "Mzo0veHN171Z"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Mzo0veHN171Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BigQuery Optimization Challenge Labs"
      ],
      "metadata": {
        "id": "RjU9WIoQBZWN"
      },
      "id": "RjU9WIoQBZWN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 1 (15 mins)"
      ],
      "metadata": {
        "id": "Mjy5r6eVvCDG"
      },
      "id": "Mjy5r6eVvCDG"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**✉️ Inbox (1): New email!**\n",
        "\n",
        "Subject: **URGENT!** Performance Degradation on Critical Sales Analysis Query\n",
        "\n",
        "\n",
        ">Dear Google Customer Engineer,\n",
        ">\n",
        "> <br>\n",
        ">\n",
        ">We are experiencing a severe performance issue with a critical SQL query used for our business review sales dashboard that our whole C level uses on a daily basis.\n",
        ">\n",
        ">Our C level **cannot afford to wait more than 30 secs** to load this SQL in the dashboard which shows data up to the last minute of sales.\n",
        ">\n",
        ">This is a new query and we thought that BigQuery would perform much better than what we are seeing now. We might need to revisit our data warehouse solution if we do not get this fixed soon.\n",
        ">\n",
        ">After some internal investigation, here is what our team has noted:\n",
        ">\n",
        ">*\"We suspect the slow performance is due to the poor performance of BQ itself, especially considering the complex UNION ALL structure, the filtering conditions (REGEXP_CONTAINS), and potentially the implicit cross-join in the second UNION ALL branch (where a join condition seems to be missing or commented out). We could try to increase slots, but we have no additional budget.\"*\n",
        ">\n",
        ">Please review the SQL below and table schemas and advise on the most effective tuning strategies (e.g., indexing, query restructuring, partitioning optimization).\n",
        ">\n",
        ">We look forward to your quick analysis and recommendations.\n",
        ">\n",
        ">\n",
        ">P.S: We need to make sure we are powering our dashboard with these columns: c_last_name, c_first_name, ca_city, bought_something.ss_ticket_number, bought_something.amt, bought_something.profit\n",
        ">\n",
        ">Thank you,\n",
        ">\n",
        "><br>\n",
        ">\n",
        ">**John Googliani**\n",
        ">\n",
        ">Data Analytics Team Lead\n",
        ">\n",
        ">The G Company"
      ],
      "metadata": {
        "id": "qTD3MNRy_gEs"
      },
      "id": "qTD3MNRy_gEs"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rules:\n",
        "\n",
        "*   Do **not** increase the size of the given BQ slot reservations\n",
        "* Do not run the queries on this lab using BQ on-demand\n",
        "*   You can apply **any technique** you know for improving SQL queries (Just do not cache it! No cheating either! 😅)\n",
        "*   You can **physically modify any table** for clustering, partitioning, etc. The **only exceptions are the large fact tables (`store_sales`, `web_sales`, `catalog_sales`)** for which we are providing a few options for you to use and reference in your SQL in order to save time.\n",
        "*   Most importantly... **Have Fun!!** 😃\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "saoN3xg3zJYh"
      },
      "id": "saoN3xg3zJYh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BQ Setup"
      ],
      "metadata": {
        "id": "czz2LrquW2D-"
      },
      "id": "czz2LrquW2D-"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sql = f\"\"\"DROP RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "md5NpLZln30U"
      },
      "id": "md5NpLZln30U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"CREATE RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\n",
        "            OPTIONS (edition = \"enterprise\",\n",
        "                     slot_capacity = 0,\n",
        "                     autoscale_max_slots = 200);\n",
        "  \"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "B_563AtkgwSD"
      },
      "id": "B_563AtkgwSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"CREATE ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\n",
        "            OPTIONS(assignee = \"projects/{PROJECT_ID}\",\n",
        "                    job_type = \"QUERY\");\n",
        "  \"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "Sg1obGyhgv_T"
      },
      "id": "Sg1obGyhgv_T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER PROJECT `{PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        " `region-US.query_runtime` = NULL\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "555ldbgG-zip"
      },
      "id": "555ldbgG-zip",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "CREATE SCHEMA IF NOT EXISTS `{PROJECT_ID}.{YOUR_WORKING_DATASET}`\n",
        "\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "-iNwX-bnPPLN"
      },
      "id": "-iNwX-bnPPLN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Case Study"
      ],
      "metadata": {
        "id": "CHyi-B_QBYw5"
      },
      "id": "CHyi-B_QBYw5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BAD Query"
      ],
      "metadata": {
        "id": "1bsHlJFOBdz7"
      },
      "id": "1bsHlJFOBdz7"
    },
    {
      "cell_type": "code",
      "source": [
        "opt_run_history = {}"
      ],
      "metadata": {
        "id": "67InQt4kcUo2"
      },
      "id": "67InQt4kcUo2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  /*\n",
        "  (1) SELECTING MORE COLUMNS THAN THE NECESSARY RESULTING IN SLOWER QUERY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "  */\n",
        "  * --ADDED (1)\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      d_dow, -- ADDED (4)\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales` store_sales,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.date_dim` date_dim,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store` store,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.household_demographics` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      /*\n",
        "      (2) REMOVED A JOIN CLAUSE RESULTING IN CROSS JOIN\n",
        "      */\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      /*\n",
        "      (4) REPLACED THE FILTER TO ALLOW MORE UNECESSARY DATA RESULTING IN MORE DATA TO BE AGGREGATE AND FILTERED OUT LATER ON IN THE QUERY\n",
        "      AND date_dim.d_dow = 1\n",
        "      */\n",
        "      AND date_dim.d_dow = 1\n",
        "      /*\n",
        "      (3) REPLACEING A WHERE FOR A STRING USING REGEXP_CONTAINS INSTEAD RESULTING IN A MUICH LESS INEFFICIENT QUERY\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      */\n",
        "      AND REGEXP_CONTAINS(CAST(date_dim.d_date AS STRING), r'^(2000|2001|2002)-') -- ADDED (3)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      d_dow -- ADDED (4)\n",
        "UNION ALL -- ADDED (4)\n",
        "SELECT -- ADDED (4)\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      d_dow, -- ADDED (4)\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales` store_sales,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.date_dim` date_dim,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store` store,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.household_demographics` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      /*\n",
        "      (2) REMOVED A JOIN CLAUSE RESULTING IN CROSS JOIN\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      */\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      /*\n",
        "      (4) ADDED TO THE FILTER TO ALLOW MORE UNECESSARY DATA + USING UNECESSARY UNION ALL RESULTING IN MORE DATA TO BE AGGREGATE AND FILTERED OUT LATER ON IN THE QUERY\n",
        "      AND date_dim.d_dow = 1\n",
        "      */\n",
        "      AND date_dim.d_dow = 2 -- ADDED (4)\n",
        "      /*\n",
        "      (3) REPLACING A WHERE FOR A STRING USING REGEXP_CONTAINS INSTEAD RESULTING IN A MUICH LESS INEFFICIENT QUERY\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      */\n",
        "      AND REGEXP_CONTAINS(CAST(date_dim.d_date AS STRING), r'^(2000|2001|2002)-') -- ADDED (3)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      d_dow -- ADDED (4)\n",
        "    /*\n",
        "    (5) ADDING A USELESS ORDER BY RESULTING IN A SLOWER QUERY THAT WILL CHANGE THE ORDER IN THE OUTER QUERY\n",
        "    */\n",
        "    ORDER BY 1 DESC, 2 DESC -- ADDED (5)\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer` customer,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer_address` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND bought_something.d_dow = 1 -- ADDED (4)\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['bad_query']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "job_badsql, opt_run_history['bad_query'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "yONO9PO_I1LL"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yONO9PO_I1LL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Work Here"
      ],
      "metadata": {
        "id": "pGMT9KqkGpOJ"
      },
      "id": "pGMT9KqkGpOJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query ReWrite"
      ],
      "metadata": {
        "id": "dlecOGUnatY8"
      },
      "id": "dlecOGUnatY8"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales` store_sales,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.date_dim` date_dim,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store` store,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.household_demographics` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      AND date_dim.d_dow = 1\n",
        "      AND 1 = 1\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer` customer,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer_address` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['sql_rewrite']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "job_step1sql, opt_run_history['sql_rewrite'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "o96PNH1oH8lf"
      },
      "id": "o96PNH1oH8lf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_bq_jobs(job_badsql, job_step1sql)"
      ],
      "metadata": {
        "id": "4bAtUwLH9t7r"
      },
      "id": "4bAtUwLH9t7r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Partitioning + Clustering to store_sales table"
      ],
      "metadata": {
        "id": "35xErO94a0SR"
      },
      "id": "35xErO94a0SR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_sales_v4`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_sales_v4`\n",
        "(\n",
        "  ss_sold_date_sk INT64,\n",
        "  ss_sold_time_sk INT64,\n",
        "  ss_item_sk INT64,\n",
        "  ss_customer_sk INT64,\n",
        "  ss_cdemo_sk INT64,\n",
        "  ss_hdemo_sk INT64,\n",
        "  ss_addr_sk INT64,\n",
        "  ss_store_sk INT64,\n",
        "  ss_promo_sk INT64,\n",
        "  ss_ticket_number INT64,\n",
        "  ss_quantity INT64,\n",
        "  ss_wholesale_cost FLOAT64,\n",
        "  ss_list_price FLOAT64,\n",
        "  ss_sales_price FLOAT64,\n",
        "  ss_ext_discount_amt FLOAT64,\n",
        "  ss_ext_sales_price FLOAT64,\n",
        "  ss_ext_wholesale_cost FLOAT64,\n",
        "  ss_ext_list_price FLOAT64,\n",
        "  ss_ext_tax FLOAT64,\n",
        "  ss_coupon_amt FLOAT64,\n",
        "  ss_net_paid FLOAT64,\n",
        "  ss_net_paid_inc_tax FLOAT64,\n",
        "  ss_net_profit FLOAT64\n",
        ")\n",
        "PARTITION BY\n",
        "  RANGE_BUCKET(ss_sold_time_sk, GENERATE_ARRAY(28800, 75599, 30))\n",
        "OPTIONS(\n",
        "  description = 'Partitioned on ss_sold_time_sk column'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)\n"
      ],
      "metadata": {
        "id": "q30K2CDwbwJt"
      },
      "id": "q30K2CDwbwJt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not executing the INSERT, we will use a pre-created table instead.\n",
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_sales_v4`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales`\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8lTgHWd1bv6O"
      },
      "id": "8lTgHWd1bv6O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales_v4` store_sales,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.date_dim` date_dim,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store` store,\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.household_demographics` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      AND date_dim.d_dow = 1\n",
        "      AND 1 = 1\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer` customer,\n",
        "  `{PROJECT_ID}.{LINKED_DATASET_ID}.customer_address` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['fact_clus_and_part']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "job_step2sql, opt_run_history['fact_clus_and_part'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "Y0qpOEfrmZF-"
      },
      "id": "Y0qpOEfrmZF-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_bq_jobs(job_badsql, job_step2sql)"
      ],
      "metadata": {
        "id": "8hrkPRXkmxAu"
      },
      "id": "8hrkPRXkmxAu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustering the tables customer and customer_address"
      ],
      "metadata": {
        "id": "Y6t8YcWIegq4"
      },
      "id": "Y6t8YcWIegq4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1`\n",
        "(\n",
        "  ca_address_sk INT64,\n",
        "  ca_address_id STRING,\n",
        "  ca_street_number STRING,\n",
        "  ca_street_name STRING,\n",
        "  ca_street_type STRING,\n",
        "  ca_suite_number STRING,\n",
        "  ca_city STRING,\n",
        "  ca_county STRING,\n",
        "  ca_state STRING,\n",
        "  ca_zip STRING,\n",
        "  ca_country STRING,\n",
        "  ca_gmt_offset FLOAT64,\n",
        "  ca_location_type STRING\n",
        ")\n",
        "CLUSTER BY\n",
        "  ca_address_sk\n",
        "OPTIONS(\n",
        "  description = 'Clustered on ca_address_sk column'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "OBVwIBHjdFAv"
      },
      "id": "OBVwIBHjdFAv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.customer_address`\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "pC3jSCYEfRq3"
      },
      "id": "pC3jSCYEfRq3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1`\n",
        "(\n",
        "  c_customer_sk INT64,\n",
        "  c_customer_id STRING,\n",
        "  c_current_cdemo_sk INT64,\n",
        "  c_current_hdemo_sk INT64,\n",
        "  c_current_addr_sk INT64,\n",
        "  c_first_shipto_date_sk INT64,\n",
        "  c_first_sales_date_sk INT64,\n",
        "  c_salutation STRING,\n",
        "  c_first_name STRING,\n",
        "  c_last_name STRING,\n",
        "  c_preferred_cust_flag STRING,\n",
        "  c_birth_day INT64,\n",
        "  c_birth_month INT64,\n",
        "  c_birth_year INT64,\n",
        "  c_birth_country STRING,\n",
        "  c_login STRING,\n",
        "  c_email_address STRING,\n",
        "  c_last_review_date_sk INT64\n",
        ")\n",
        "CLUSTER BY\n",
        "  c_customer_sk, c_current_addr_sk\n",
        "OPTIONS(\n",
        "  description = 'Clustered on c_customer_sk, c_current_addr_sk columns'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "iiV041vLdTRl"
      },
      "id": "iiV041vLdTRl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.customer`\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "x-3XBgZ_fget"
      },
      "id": "x-3XBgZ_fget",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1`\n",
        "(\n",
        "  d_date_sk INT64,\n",
        "  d_date_id STRING,\n",
        "  d_date DATE,\n",
        "  d_month_seq INT64,\n",
        "  d_week_seq INT64,\n",
        "  d_quarter_seq INT64,\n",
        "  d_year INT64,\n",
        "  d_dow INT64,\n",
        "  d_moy INT64,\n",
        "  d_dom INT64,\n",
        "  d_qoy INT64,\n",
        "  d_fy_year INT64,\n",
        "  d_fy_quarter_seq INT64,\n",
        "  d_fy_week_seq INT64,\n",
        "  d_day_name STRING,\n",
        "  d_quarter_name STRING,\n",
        "  d_holiday STRING,\n",
        "  d_weekend STRING,\n",
        "  d_following_holiday STRING,\n",
        "  d_first_dom INT64,\n",
        "  d_last_dom INT64,\n",
        "  d_same_day_ly INT64,\n",
        "  d_same_day_lq INT64,\n",
        "  d_current_day STRING,\n",
        "  d_current_week STRING,\n",
        "  d_current_month STRING,\n",
        "  d_current_quarter STRING,\n",
        "  d_current_year STRING,\n",
        "  PRIMARY KEY (d_date_sk) NOT ENFORCED\n",
        ")\n",
        "CLUSTER BY d_date_sk\n",
        "OPTIONS(\n",
        "  description = 'Clustered on d_date_sk columns'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "BL1TiCmCo7qa"
      },
      "id": "BL1TiCmCo7qa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.date_dim`\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "2EZB2u5rqn3o"
      },
      "id": "2EZB2u5rqn3o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1`\n",
        "(\n",
        "  s_store_sk INT64,\n",
        "  s_store_id STRING,\n",
        "  s_rec_start_date DATE,\n",
        "  s_rec_end_date DATE,\n",
        "  s_closed_date_sk INT64,\n",
        "  s_store_name STRING,\n",
        "  s_number_employees INT64,\n",
        "  s_floor_space INT64,\n",
        "  s_hours STRING,\n",
        "  s_manager STRING,\n",
        "  s_market_id INT64,\n",
        "  s_geography_class STRING,\n",
        "  s_market_desc STRING,\n",
        "  s_market_manager STRING,\n",
        "  s_division_id INT64,\n",
        "  s_division_name STRING,\n",
        "  s_company_id INT64,\n",
        "  s_company_name STRING,\n",
        "  s_street_number STRING,\n",
        "  s_street_name STRING,\n",
        "  s_street_type STRING,\n",
        "  s_suite_number STRING,\n",
        "  s_city STRING,\n",
        "  s_county STRING,\n",
        "  s_state STRING,\n",
        "  s_zip STRING,\n",
        "  s_country STRING,\n",
        "  s_gmt_offset FLOAT64,\n",
        "  s_tax_percentage FLOAT64\n",
        ")\n",
        "CLUSTER BY s_store_sk\n",
        "OPTIONS(\n",
        "  description = 'Clustered on s_store_sk column'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "Yg1ZqrVdo8Nk"
      },
      "id": "Yg1ZqrVdo8Nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.store`\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "QE9SbU14uEwF"
      },
      "id": "QE9SbU14uEwF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "sql = f\"\"\"CREATE TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1`\n",
        "(\n",
        "  hd_demo_sk INT64,\n",
        "  hd_income_band_sk INT64,\n",
        "  hd_buy_potential STRING,\n",
        "  hd_dep_count INT64,\n",
        "  hd_vehicle_count INT64\n",
        ")\n",
        "CLUSTER BY hd_demo_sk\n",
        "OPTIONS(\n",
        "  description = 'Clustered on hd_demo_sk column'\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "HId-fhP-vvx7"
      },
      "id": "HId-fhP-vvx7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"insert into `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1`\n",
        "select * from `{PROJECT_ID}.{LINKED_DATASET_ID}.household_demographics`\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "Z5LWa75Dvvm7"
      },
      "id": "Z5LWa75Dvvm7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales_v4` store_sales,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1` date_dim,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1` store,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      AND date_dim.d_dow = 1\n",
        "      AND 1 = 1\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1` customer,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['dimension_clu_plus_part']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "job_step3sql, opt_run_history['dimension_clu_plus_part'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "Xm7hC8gGeraw"
      },
      "id": "Xm7hC8gGeraw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_bq_jobs(job_badsql, job_step3sql)"
      ],
      "metadata": {
        "id": "durXhrFMf7Ik"
      },
      "id": "durXhrFMf7Ik",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add PKs and FKs"
      ],
      "metadata": {
        "id": "RWYGe0QfFOLy"
      },
      "id": "RWYGe0QfFOLy"
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_sales_v4`\n",
        "ADD PRIMARY KEY (ss_item_sk, ss_ticket_number) NOT ENFORCED\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "JqcxUGHqrE-k"
      },
      "id": "JqcxUGHqrE-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1`\n",
        "ADD PRIMARY KEY (c_customer_sk) NOT ENFORCED\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "J3YCrq0NFR-z"
      },
      "id": "J3YCrq0NFR-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1`\n",
        "ADD PRIMARY KEY (ca_address_sk) NOT ENFORCED\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "QnD1T1d6FVgB"
      },
      "id": "QnD1T1d6FVgB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1`\n",
        "ADD PRIMARY KEY (s_store_sk) NOT ENFORCED\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "m5qq8bt3IwsE"
      },
      "id": "m5qq8bt3IwsE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER TABLE `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1`\n",
        "ADD PRIMARY KEY (hd_demo_sk) NOT ENFORCED\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "SWCLVJSUTg4_"
      },
      "id": "SWCLVJSUTg4_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER table `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales_v4`\n",
        "ADD FOREIGN KEY(ss_sold_date_sk) references `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1`(d_date_sk)\n",
        " NOT ENFORCED,\n",
        "ADD FOREIGN KEY(ss_store_sk) references `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1`(s_store_sk)\n",
        " NOT ENFORCED,\n",
        "ADD FOREIGN KEY(ss_hdemo_sk) references `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1`(hd_demo_sk)\n",
        " NOT ENFORCED\"\"\"\n",
        "\n",
        "# Removing is because our LINKED dataset is READ-ONLY. So, we cannot create the PKs/FKs\n",
        "# run_bq_query(sql)\n",
        "\n"
      ],
      "metadata": {
        "id": "XqdTZwYAGXix"
      },
      "id": "XqdTZwYAGXix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales_v4` store_sales,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1` date_dim,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1` store,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      AND date_dim.d_dow = 1\n",
        "      AND 1 = 1\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1` customer,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['adding_dim_pks']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# Removing is because our LINKED dataset is READ-ONLY. So, we cannot create the PKs/FKs\n",
        "job_step4sql, opt_run_history['adding_dim_pks'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "e32xHpLOUXM-"
      },
      "id": "e32xHpLOUXM-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_bq_jobs(job_badsql, job_step4sql)"
      ],
      "metadata": {
        "id": "Ypa2kSZLUcGY"
      },
      "id": "Ypa2kSZLUcGY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advanced Query RunTime (no gain observed)"
      ],
      "metadata": {
        "id": "xuQQKgzyJ875"
      },
      "id": "xuQQKgzyJ875"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf62a86c"
      },
      "source": [
        "sql = f\"\"\"ALTER PROJECT `{PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        " `region-US.query_runtime` = \"advanced\"\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)\n"
      ],
      "id": "cf62a86c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "\n",
        "sql = f\"\"\"\n",
        "SELECT\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number,\n",
        "  bought_something.amt,\n",
        "  bought_something.profit\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk,\n",
        "      sum(ss_ext_sales_price) AS amt,\n",
        "      sum(ss_net_profit) AS profit\n",
        "    FROM\n",
        "      `{PROJECT_ID}.{LINKED_DATASET_ID}.store_sales_v4` store_sales,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.date_dim_v1` date_dim,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.store_v1` store,\n",
        "      `{PROJECT_ID}.{YOUR_WORKING_DATASET}.household_demographics_v1` household_demographics\n",
        "    WHERE\n",
        "      store_sales.ss_sold_date_sk = date_dim.d_date_sk\n",
        "      AND store_sales.ss_store_sk = store.s_store_sk\n",
        "      AND store_sales.ss_hdemo_sk = household_demographics.hd_demo_sk\n",
        "      AND (\n",
        "        household_demographics.hd_dep_count = 5\n",
        "        OR household_demographics.hd_vehicle_count > 2\n",
        "      )\n",
        "      AND date_dim.d_dow = 1\n",
        "      AND 1 = 1\n",
        "      AND date_dim.d_year IN (2000, 2001, 2002)\n",
        "      AND store.s_county IN (\n",
        "        'Williamson County',\n",
        "        'Franklin Parish',\n",
        "        'Bronx County',\n",
        "        'Orange County'\n",
        "      )\n",
        "    GROUP BY\n",
        "      ss_ticket_number,\n",
        "      ss_customer_sk\n",
        "  ) AS bought_something,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_v1` customer,\n",
        "  `{PROJECT_ID}.{YOUR_WORKING_DATASET}.customer_address_v1` customer_address\n",
        "WHERE\n",
        "  bought_something.ss_customer_sk = customer.c_customer_sk\n",
        "  AND customer.c_current_addr_sk = customer_address.ca_address_sk\n",
        "  AND (\n",
        "    substr(ca_zip, 1, 5) IN (\n",
        "      '85562',\n",
        "      '86375',\n",
        "      '87063',\n",
        "      '85888',\n",
        "      '82981',\n",
        "      '82173',\n",
        "      '81980',\n",
        "      '84000',\n",
        "      '85966',\n",
        "      '85994'\n",
        "    )\n",
        "    OR ca_state IN (\n",
        "      'CA',\n",
        "      'WA',\n",
        "      'GA'\n",
        "    )\n",
        "    OR bought_something.profit > 500\n",
        "  )\n",
        "ORDER BY\n",
        "  c_last_name,\n",
        "  c_first_name,\n",
        "  ca_city,\n",
        "  bought_something.ss_ticket_number\n",
        "\"\"\"\n",
        "\n",
        "sleep(120)\n",
        "\n",
        "# Drop if exists\n",
        "try:\n",
        "  del opt_run_history['advanced_runtime']\n",
        "except:\n",
        "  pass\n",
        "\n",
        "job_step5sql, opt_run_history['advanced_runtime'] = run_bq_query(sql)"
      ],
      "metadata": {
        "id": "GF7T0JZsK-8j"
      },
      "id": "GF7T0JZsK-8j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compare_bq_jobs(job_badsql, job_step5sql)"
      ],
      "metadata": {
        "id": "SZ9Jq7LzLbW4"
      },
      "id": "SZ9Jq7LzLbW4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(opt_run_history)\n",
        "df = df.T\n",
        "df = df.reset_index()\n",
        "df"
      ],
      "metadata": {
        "id": "7NwtuaG-lPgO"
      },
      "id": "7NwtuaG-lPgO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.bar(df, x='index', y='duration', title=\"Results Comparison\",  labels={'index':'Performance Technique', 'duration': 'Duration (sec)'})\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "JAxkkzZ6wLQl"
      },
      "id": "JAxkkzZ6wLQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 1 Cleanup"
      ],
      "metadata": {
        "id": "oYWwoIYs_CKH"
      },
      "id": "oYWwoIYs_CKH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to uncomment the code below."
      ],
      "metadata": {
        "id": "TdQncrwP_q51"
      },
      "id": "TdQncrwP_q51"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sql = f\"\"\"DROP RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "WeBKe5Mh_EHH"
      },
      "id": "WeBKe5Mh_EHH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql = f\"\"\"ALTER PROJECT `{PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        " `region-US.query_runtime` = NULL\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "I4YstoQs_Qie"
      },
      "id": "I4YstoQs_Qie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 2 (30 mins)\n",
        "\n",
        "---\n",
        "\n",
        "### **✉️ Inbox (1) - New email!:**\n",
        "\n",
        "**Subject:** *Re: URGENT: Performance Degradation on Critical Sales Analysis Query // **New Performance Issue: Multi-stage Customer Analysis Query***\n",
        "\n",
        "\n",
        "> *Dear Google Customer Engineer,*\n",
        ">\n",
        "> <br>\n",
        ">\n",
        "> *Thank you so much for your help with our sales dashboard query. Your recommendations were spot on! The C-suite is thrilled with the new performance, and the dashboard now loads well under our 30-second target.*\n",
        ">\n",
        "> <br>\n",
        ">\n",
        "> *Unfortunately, we've hit another performance bottleneck with a different, more complex query used for a critical customer behavior analysis report. The current execution time is over **12 minutes**, which is completely unworkable for our analysts.*\n",
        ">\n",
        "> <br>\n",
        ">\n",
        "> *This query performs a multi-stage analysis to help us understand our best customers. In short, it does the following:*\n",
        ">\n",
        "> 1.  *Finds our most **frequently sold items** in stores between 1999 and 2002.*\n",
        "> 2.  *Identifies our **top store customers** (the 5th percentile) based on their total sales during that same period.*\n",
        "> 3.  *Finally, it calculates the total **web and catalog sales** in December 2002, specifically for these top customers buying those frequent items.*\n",
        ">\n",
        "> <br>\n",
        ">\n",
        "> *Our goal is to get this query to run in **under a minute and a half**.*\n",
        ">\n",
        "> *Could you please review this new query and provide your optimization expertise? Your help on the last one was invaluable, and we'd appreciate your support again here.*\n",
        ">\n",
        "> <br>\n",
        ">\n",
        "> *Thank you,*\n",
        ">\n",
        "> ***John Googliani***\n",
        ">\n",
        "> *Data Analytics Team Lead*\n",
        ">\n",
        "> *The G Company*"
      ],
      "metadata": {
        "id": "ovhJTbY7PUBP"
      },
      "id": "ovhJTbY7PUBP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BQ Setup"
      ],
      "metadata": {
        "id": "bb7QRl0HPclv"
      },
      "id": "bb7QRl0HPclv"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop if exists\n",
        "sql = f\"\"\"DROP ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sql = f\"\"\"DROP RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "7l-HD-3hPYmG"
      },
      "id": "7l-HD-3hPYmG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a reservation with 0 baseline and 1,000 max autoscale slots.,\n",
        "sql = f\"\"\"CREATE RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\n",
        "            OPTIONS (edition = \"enterprise\",\n",
        "                     slot_capacity = 0,\n",
        "                     autoscale_max_slots = 1000);\n",
        "  \"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "hhMyklRSPk8Y"
      },
      "id": "hhMyklRSPk8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign the current project to the new reservation\n",
        "sql = f\"\"\"CREATE ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\n",
        "            OPTIONS(assignee = \"projects/{PROJECT_ID}\",\n",
        "                    job_type = \"QUERY\");\n",
        "  \"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "_TgW3OaFQADr"
      },
      "id": "_TgW3OaFQADr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable BQ advanced query runtime\n",
        "sql = f\"\"\"ALTER PROJECT `{PROJECT_ID}`\n",
        "SET OPTIONS (\n",
        " `region-{REGION}.query_runtime` = NULL\n",
        ")\"\"\"\n",
        "\n",
        "run_bq_query(sql)"
      ],
      "metadata": {
        "id": "hFSDl1cUQGHD"
      },
      "id": "hFSDl1cUQGHD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store executions\n",
        "opt_run_history_c2 = {}"
      ],
      "metadata": {
        "id": "YdBJq0HLH1Lp"
      },
      "id": "YdBJq0HLH1Lp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get started\n",
        "\n",
        "**Challenge rules:**\n",
        "- Use a 1,000 max slot Enterprise reservation, no idle slot sharing\n",
        "- No caching or materialization\n",
        "- You can modify the query as long as the business logic and query results are the same\n",
        "- You can find the tables in the dataset [linked earlier in this notebook](https://console.cloud.google.com/bigquery/analytics-hub/exchanges/projects/720965328418/locations/us/dataExchanges/argolis_shared_data_1840b70ffcb/listings/bigquery_optimization_lab_199a6715d4a).\n",
        "- You can create your own versions of the tables (e.g. with different partitioning or clustering specs) as needed. The fact tables (`catalog_sales`,`store_sales`, `web_sales`) are very large and it will take a long time to recreate. To speed things up, you can use any of the already partitioned/clustered tables with the suffix `_v...`.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "####Original query:\n",
        "\n",
        "```sql\n",
        "WITH\n",
        "  frequent_ss_items_1999 AS (\n",
        "    # Find frequently sold items in 1999\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.store_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`,\n",
        "      `<your_project_id>.<your_dataset_id>.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year IN (1999)\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2000 AS (\n",
        "    # Find frequently sold items in 2000\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.store_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`,\n",
        "      `<your_project_id>.<your_dataset_id>.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2000\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2001 AS (\n",
        "    # Find frequently sold items in 2001\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.store_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`,\n",
        "      `<your_project_id>.<your_dataset_id>.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2001\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2002 AS (\n",
        "    # Find frequently sold items in 2002\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.store_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`,\n",
        "      `<your_project_id>.<your_dataset_id>.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2002\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items AS (\n",
        "    # Combine the frequently sold items in the years above\n",
        "    SELECT * FROM frequent_ss_items_1999\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2000\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2001\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2002\n",
        "  ),\n",
        "  max_store_sales AS (\n",
        "    # Calculates the max store sales per customer in the same time period\n",
        "    SELECT\n",
        "      MAX(csales) tpcds_cmax\n",
        "    FROM\n",
        "      (\n",
        "        SELECT\n",
        "          c_customer_sk,\n",
        "          SUM(ss_quantity * ss_sales_price) csales\n",
        "        FROM\n",
        "          `<your_project_id>.<your_dataset_id>.store_sales`\n",
        "        CROSS JOIN\n",
        "          `<your_project_id>.<your_dataset_id>.date_dim`\n",
        "        CROSS JOIN\n",
        "          `<your_project_id>.<your_dataset_id>.customer`\n",
        "        WHERE\n",
        "          ss_customer_sk = c_customer_sk\n",
        "          AND ss_sold_date_sk = d_date_sk\n",
        "          AND d_year IN (\n",
        "            1999,\n",
        "            1999 + 1,\n",
        "            1999 + 2,\n",
        "            1999 + 3)\n",
        "        GROUP BY\n",
        "          c_customer_sk\n",
        "      )\n",
        "  ),\n",
        "  best_ss_customer AS (\n",
        "    # Finds the best store customers in the top 5% of sales\n",
        "    SELECT\n",
        "      c_customer_sk,\n",
        "      SUM(ss_quantity * ss_sales_price) ssales\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.store_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.customer`\n",
        "    WHERE\n",
        "      ss_customer_sk = c_customer_sk\n",
        "    GROUP BY\n",
        "      c_customer_sk\n",
        "    HAVING\n",
        "      SUM(ss_quantity * ss_sales_price) > (95 / 100.0) * (SELECT * FROM max_store_sales)\n",
        "  )\n",
        "\n",
        "# Calculate web and catalog sales from December 2002\n",
        "# made by the best store customers buying the most frequent store items.\n",
        "SELECT\n",
        "  SUM(sales)\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      cs_quantity * cs_list_price sales\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.catalog_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`\n",
        "    WHERE\n",
        "      d_year = 2002\n",
        "      AND d_moy = 12\n",
        "      AND cs_sold_date_sk = d_date_sk\n",
        "      AND cs_item_sk IN (\n",
        "        SELECT\n",
        "          item_sk\n",
        "        FROM\n",
        "          frequent_ss_items\n",
        "      )\n",
        "      AND cs_bill_customer_sk IN (\n",
        "        SELECT\n",
        "          c_customer_sk\n",
        "        FROM\n",
        "          best_ss_customer\n",
        "      )\n",
        "    UNION ALL\n",
        "    SELECT\n",
        "      ws_quantity * ws_list_price sales\n",
        "    FROM\n",
        "      `<your_project_id>.<your_dataset_id>.web_sales`,\n",
        "      `<your_project_id>.<your_dataset_id>.date_dim`\n",
        "    WHERE\n",
        "      d_year = 2002\n",
        "      AND d_moy = 12\n",
        "      AND ws_sold_date_sk = d_date_sk\n",
        "      AND ws_item_sk IN (\n",
        "        SELECT\n",
        "          item_sk\n",
        "        FROM\n",
        "          frequent_ss_items\n",
        "      )\n",
        "      AND ws_bill_customer_sk IN (\n",
        "        SELECT\n",
        "          c_customer_sk\n",
        "        FROM\n",
        "          best_ss_customer\n",
        "      )\n",
        "  )\n",
        "LIMIT\n",
        "  100;\n",
        "```\n",
        "---\n",
        "\n",
        "**Note: this initial query runs on non-partitioned, non-clustered data and it will take about 12 minues. You don't need to run it during the lab**"
      ],
      "metadata": {
        "id": "c7dO-ZCd5dHm"
      },
      "id": "c7dO-ZCd5dHm"
    },
    {
      "cell_type": "code",
      "source": [
        "# [Query 0] Initial attempt with unpartitioned dataset\n",
        "# DO NOT RUN DURING THE LAB - it will take more than 10 min\n",
        "\n",
        "C_DATASET_ID = f\"{PROJECT_ID}.{LINKED_DATASET_ID}\"\n",
        "\n",
        "q0_query = f\"\"\"WITH\n",
        "  frequent_ss_items_1999 AS (\n",
        "    # Find frequently sold items in 1999\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.store_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`,\n",
        "      `{C_DATASET_ID}.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year IN (1999)\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2000 AS (\n",
        "    # Find frequently sold items in 2000\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.store_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`,\n",
        "      `{C_DATASET_ID}.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2000\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2001 AS (\n",
        "    # Find frequently sold items in 2001\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.store_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`,\n",
        "      `{C_DATASET_ID}.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2001\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items_2002 AS (\n",
        "    # Find frequently sold items in 2002\n",
        "    SELECT\n",
        "      i_item_desc,\n",
        "      i_item_sk item_sk,\n",
        "      d_date solddate,\n",
        "      COUNT(*) cnt\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.store_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`,\n",
        "      `{C_DATASET_ID}.item`\n",
        "    WHERE\n",
        "      ss_sold_date_sk = d_date_sk\n",
        "      AND ss_item_sk = i_item_sk\n",
        "      AND d_year = 2002\n",
        "    GROUP BY\n",
        "      i_item_desc,\n",
        "      i_item_sk,\n",
        "      d_date\n",
        "    HAVING\n",
        "      COUNT(*) > 4\n",
        "  ),\n",
        "  frequent_ss_items AS (\n",
        "    # Combine the frequently sold items in the years above\n",
        "    SELECT * FROM frequent_ss_items_1999\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2000\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2001\n",
        "    UNION ALL\n",
        "    SELECT * FROM frequent_ss_items_2002\n",
        "  ),\n",
        "  max_store_sales AS (\n",
        "    # Calculates the max store sales per customer in the same time period\n",
        "    SELECT\n",
        "      MAX(csales) tpcds_cmax\n",
        "    FROM\n",
        "      (\n",
        "        SELECT\n",
        "          c_customer_sk,\n",
        "          SUM(ss_quantity * ss_sales_price) csales\n",
        "        FROM\n",
        "          `{C_DATASET_ID}.store_sales`\n",
        "        CROSS JOIN\n",
        "          `{C_DATASET_ID}.date_dim`\n",
        "        CROSS JOIN\n",
        "          `{C_DATASET_ID}.customer`\n",
        "        WHERE\n",
        "          ss_customer_sk = c_customer_sk\n",
        "          AND ss_sold_date_sk = d_date_sk\n",
        "          AND d_year IN (\n",
        "            1999,\n",
        "            1999 + 1,\n",
        "            1999 + 2,\n",
        "            1999 + 3)\n",
        "        GROUP BY\n",
        "          c_customer_sk\n",
        "      )\n",
        "  ),\n",
        "  best_ss_customer AS (\n",
        "    # Finds the best store customers in the top 5% of sales\n",
        "    SELECT\n",
        "      c_customer_sk,\n",
        "      SUM(ss_quantity * ss_sales_price) ssales\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.store_sales`,\n",
        "      `{C_DATASET_ID}.customer`\n",
        "    WHERE\n",
        "      ss_customer_sk = c_customer_sk\n",
        "    GROUP BY\n",
        "      c_customer_sk\n",
        "    HAVING\n",
        "      SUM(ss_quantity * ss_sales_price) > (95 / 100.0) * (SELECT * FROM max_store_sales)\n",
        "  )\n",
        "\n",
        "# Calculate web and catalog sales from December 2002\n",
        "# made by the best store customers buying the most frequent store items.\n",
        "SELECT\n",
        "  SUM(sales)\n",
        "FROM\n",
        "  (\n",
        "    SELECT\n",
        "      cs_quantity * cs_list_price sales\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.catalog_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`\n",
        "    WHERE\n",
        "      d_year = 2002\n",
        "      AND d_moy = 12\n",
        "      AND cs_sold_date_sk = d_date_sk\n",
        "      AND cs_item_sk IN (\n",
        "        SELECT\n",
        "          item_sk\n",
        "        FROM\n",
        "          frequent_ss_items\n",
        "      )\n",
        "      AND cs_bill_customer_sk IN (\n",
        "        SELECT\n",
        "          c_customer_sk\n",
        "        FROM\n",
        "          best_ss_customer\n",
        "      )\n",
        "    UNION ALL\n",
        "    SELECT\n",
        "      ws_quantity * ws_list_price sales\n",
        "    FROM\n",
        "      `{C_DATASET_ID}.web_sales`,\n",
        "      `{C_DATASET_ID}.date_dim`\n",
        "    WHERE\n",
        "      d_year = 2002\n",
        "      AND d_moy = 12\n",
        "      AND ws_sold_date_sk = d_date_sk\n",
        "      AND ws_item_sk IN (\n",
        "        SELECT\n",
        "          item_sk\n",
        "        FROM\n",
        "          frequent_ss_items\n",
        "      )\n",
        "      AND ws_bill_customer_sk IN (\n",
        "        SELECT\n",
        "          c_customer_sk\n",
        "        FROM\n",
        "          best_ss_customer\n",
        "      )\n",
        "  )\n",
        "LIMIT\n",
        "  100;\n",
        "\"\"\"\n",
        "\n",
        "# DO NOT RUN - takes over 10 min\n",
        "#q0, opt_run_history_c2['q0_original'] = run_bq_query(q0_query, show_query_results=True)"
      ],
      "metadata": {
        "id": "fAFoSACZ6tEx"
      },
      "id": "fAFoSACZ6tEx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of running the initial long query, we'll capture the execution stats in a\n",
        "# dictionary for comparison\n",
        "\n",
        "q0_stats = {\n",
        "    \"id\": \"2ffb94ef-165d-4ae8-a2cd-4acbd84cfcbf\",\n",
        "    \"rows_returned\": 1,\n",
        "    \"query_time\": 675.62,\n",
        "    \"bytes_scanned\": 1972736671505,\n",
        "    \"slot_ms\": 654427904\n",
        "}"
      ],
      "metadata": {
        "id": "6oi3ncw1PVR2"
      },
      "id": "6oi3ncw1PVR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Running query in project '__________', location 'us'...\n",
        "Job started. ID: 2ffb94ef-165d-4ae8-a2cd-4acbd84cfcbf\n",
        "Job 2ffb94ef-165d-4ae8-a2cd-4acbd84cfcbf finished with state: DONE\n",
        "------------------------------\n",
        " Rows Returned:    1\n",
        " Duration:      675.62 s\n",
        " Bytes Scanned:    1.8 TiB (1,972,736,671,505 bytes)\n",
        " Slot Time:      654,427,904 ms\n",
        "------------------------------\n",
        "\n",
        "Query Results (first 10 rows):\n",
        "------------------------------\n",
        "         f0_\n",
        "0  293712.02\n",
        "------------------------------\n",
        "```"
      ],
      "metadata": {
        "id": "zCcij4k-65Mr"
      },
      "id": "zCcij4k-65Mr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your Work Here:"
      ],
      "metadata": {
        "id": "2aEsOShS6YlX"
      },
      "id": "2aEsOShS6YlX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your work here..."
      ],
      "metadata": {
        "id": "wipV_E-26Zd1"
      },
      "id": "wipV_E-26Zd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Challenge 2 Cleanup"
      ],
      "metadata": {
        "id": "FW5JaZWEzd6E"
      },
      "id": "FW5JaZWEzd6E"
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop assignment and reservation if they exist\n",
        "sql = f\"\"\"DROP ASSIGNMENT `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}.{RESERVATION_NAME}-assignment`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass\n",
        "\n",
        "sql = f\"\"\"DROP RESERVATION `{PROJECT_ID}.region-{REGION}.{RESERVATION_NAME}`\"\"\"\n",
        "\n",
        "try:\n",
        "  run_bq_query(sql)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "81oHegJIzqOV"
      },
      "id": "81oHegJIzqOV",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "FINAL DA_SME_Academy_2025_BQ_Optimization_Attendee's_Notebook",
      "toc_visible": true,
      "collapsed_sections": [
        "zHkJOKAkvTOG",
        "d9SDCYvSo9Sa",
        "czz2LrquW2D-",
        "1bsHlJFOBdz7",
        "pGMT9KqkGpOJ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}